{"cells":[{"cell_type":"markdown","metadata":{"id":"Mlf0v2adFGzT"},"source":["# ニューラルネット\n","## 第6章で取り組んだニュース記事のカテゴリ分類を題材として，ニューラルネットワークでカテゴリ分類モデルを実装する．なお，この章ではPyTorch, TensorFlow, Chainerなどの機械学習プラットフォームを活用せよ．\n","\n","\n","\n","####https://nlp100.github.io/ja/ch08.html"]},{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iamtatsuki05/NLP_100/blob/NLP_100_9/NLP_100_8.ipynb)"],"metadata":{"id":"5efGfGAXJcy3"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18002,"status":"ok","timestamp":1648176685795,"user":{"displayName":"Tatsuki Okada","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12310138447043973112"},"user_tz":-540},"id":"jBYlYY_Q7o3R","outputId":"42c642ca-b3ac-4d20-f0af-b58b5aac9f09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ap0ufgsdFG2U"},"source":["# 単語ベクトルの和による特徴量\n","## 問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例xiの特徴ベクトルxiを並べた行列Xと，正解ラベルを並べた行列（ベクトル）Yを作成したい．\n","\n","X=⎛⎝⎜⎜⎜⎜x1x2…xn⎞⎠⎟⎟⎟⎟∈ℝn×d,Y=⎛⎝⎜⎜⎜⎜y1y2…yn⎞⎠⎟⎟⎟⎟∈ℕn\n","ここで，nは学習データの事例数であり，xi∈ℝdとyi∈ℕはそれぞれ，i∈{1,…,n}番目の事例の特徴量ベクトルと正解ラベルを表す． なお，今回は「ビジネス」「科学技術」「エンターテイメント」「健康」の4カテゴリ分類である．ℕ<4で4未満の自然数（0を含む）を表すことにすれば，任意の事例の正解ラベルyiはyi∈ℕ<4で表現できる． 以降では，ラベルの種類数をLで表す（今回の分類タスクではL=4である）．\n","\n","i番目の事例の特徴ベクトルxiは，次式で求める．\n","\n","xi=1Ti∑t=1Tiemb(wi,t)\n","ここで，i番目の事例はTi個の（記事見出しの）単語列(wi,1,wi,2,…,wi,Ti)から構成され，emb(w)∈ℝdは単語wに対応する単語ベクトル（次元数はd）である．すなわち，i番目の事例の記事見出しを，その見出しに含まれる単語のベクトルの平均で表現したものがxiである．今回は単語ベクトルとして，問題60でダウンロードしたものを用いればよい．300次元の単語ベクトルを用いたので，d=300である．\n","\n","i番目の事例のラベルyiは，次のように定義する．\n","\n","yi=⎧⎩⎨⎪⎪0123(記事xiが「ビジネス」カテゴリの場合)(記事xiが「科学技術」カテゴリの場合)(記事xiが「エンターテイメント」カテゴリの場合)(記事xiが「健康」カテゴリの場合)\n","なお，カテゴリ名とラベルの番号が一対一で対応付いていれば，上式の通りの対応付けでなくてもよい．\n","\n","以上の仕様に基づき，以下の行列・ベクトルを作成し，ファイルに保存せよ．\n","\n","学習データの特徴量行列: Xtrain∈ℝNt×d\n","学習データのラベルベクトル: Ytrain∈ℕNt\n","検証データの特徴量行列: Xvalid∈ℝNv×d\n","検証データのラベルベクトル: Yvalid∈ℕNv\n","評価データの特徴量行列: Xtest∈ℝNe×d\n","評価データのラベルベクトル: Ytest∈ℕNe\n","なお，Nt,Nv,Neはそれぞれ，学習データの事例数，検証データの事例数，評価データの事例数である．"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4100,"status":"ok","timestamp":1648176689889,"user":{"displayName":"Tatsuki Okada","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12310138447043973112"},"user_tz":-540},"id":"19jS0MThblNJ","outputId":"74042fb7-4e58-4e43-d86e-567121647804"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-03-25 02:51:25--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n","Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n","Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 29224203 (28M) [application/x-httpd-php]\n","Saving to: ‘NewsAggregatorDataset.zip’\n","\n","NewsAggregatorDatas 100%[===================>]  27.87M  24.8MB/s    in 1.1s    \n","\n","2022-03-25 02:51:27 (24.8 MB/s) - ‘NewsAggregatorDataset.zip’ saved [29224203/29224203]\n","\n","Archive:  NewsAggregatorDataset.zip\n","  inflating: 2pageSessions.csv       \n","   creating: __MACOSX/\n","  inflating: __MACOSX/._2pageSessions.csv  \n","  inflating: newsCorpora.csv         \n","  inflating: __MACOSX/._newsCorpora.csv  \n","  inflating: readme.txt              \n","  inflating: __MACOSX/._readme.txt   \n","SUMMARY: Dataset of references (urls) to news web pages\n","\n","DESCRIPTION: Dataset of references to news web pages collected from an online aggregator in the period from March 10 to August 10 of 2014. The resources are grouped into clusters that represent pages discussing the same news story. The dataset includes also references to web pages that point (has a link to) one of the news page in the collection.\n","\n","TAGS: web pages, news, aggregator, classification, clustering\n","\n","LICENSE: Public domain - Due to restrictions on content and use of the news sources, the corpus is limited to web references (urls) to web pages and does not include any text content. The references have been retrieved from the news aggregator through traditional web browsers. \n","\n","FILE ENCODING: UTF-8\n","\n","FORMAT: Tab delimited CSV files. \n","\n","DATA SHAPE AND STATS: 422937 news pages and divided up into:\n","\n","152746 \tnews of business category\n","108465 \tnews of science and technology category\n","115920 \tnews of business category\n"," 45615 \tnews of health category\n","\n","2076 clusters of similar news for entertainment category\n","1789 clusters of similar news for science and technology category\n","2019 clusters of similar news for business category\n","1347 clusters of similar news for health category\n","\n","References to web pages containing a link to one news included in the collection are also included. They are represented as pairs of urls corresponding to 2-page browsing sessions. The collection includes 15516 2-page browsing sessions covering 946 distinct clusters divided up into:\n","\n","6091 2-page sessions for business category\n","9425 2-page sessions for entertainment category\n","\n"," \n","\n","CONTENT\n","=======\n","\n","FILENAME #1: newsCorpora.csv (102.297.000 bytes)\n","DESCRIPTION: News pages\n","FORMAT: ID \\t TITLE \\t URL \\t PUBLISHER \\t CATEGORY \\t STORY \\t HOSTNAME \\t TIMESTAMP\n","\n","where:\n","ID\t\tNumeric ID\n","TITLE\t\tNews title \n","URL\t\tUrl\n","PUBLISHER\tPublisher name\n","CATEGORY\tNews category (b = business, t = science and technology, e = entertainment, m = health)\n","STORY\t\tAlphanumeric ID of the cluster that includes news about the same story\n","HOSTNAME\tUrl hostname\n","TIMESTAMP \tApproximate time the news was published, as the number of milliseconds since the epoch 00:00:00 GMT, January 1, 1970\n","\n","\n","FILENAME #2: 2pageSessions.csv (3.049.986 bytes)\n","DESCRIPTION: 2-page sessions\n","FORMAT: STORY \\t HOSTNAME \\t CATEGORY \\t URL\n","\n","where:\n","STORY\t\tAlphanumeric ID of the cluster that includes news about the same story\n","HOSTNAME\tUrl hostname\n","CATEGORY\tNews category (b = business, t = science and technology, e = entertainment, m = health)\n","URL\t\tTwo space-delimited urls representing a browsing session\n","\n"]}],"source":["# 学習データの特徴量行列: Xtrain\n","# 学習データのラベルベクトル: Ytrain\n","# 検証データの特徴量行列: Xvalid\n","# 検証データのラベルベクトル: Yvalid\n","# 評価データの特徴量行列: Xtest\n","# 評価データのラベルベクトル: Ytest\n","# Nt,Nv,Ne はそれぞれ，学習データの事例数，検証データの事例数，評価データの事例数．\n","\n","# 問題50のデータをもう一度作成します。\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n","!unzip NewsAggregatorDataset.zip\n","f = open('readme.txt', 'r')\n","data = f.read()\n","print(data)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"I-aV9-2idTUF","executionInfo":{"status":"ok","timestamp":1648176692878,"user_tz":-540,"elapsed":2992,"user":{"displayName":"Tatsuki Okada","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12310138447043973112"}}},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('/content/newsCorpora.csv' , header=None , sep='\\t' , names=['ID' , 'TITLE' , 'URL' , 'PUBLISHER' , 'CATEGORY' , 'STORY' , 'HOSTNAME' , 'TIMESTAMP'])\n","df = df.loc[df['PUBLISHER'].isin(['Reuters' , 'Huffington Post' , 'Businessweek' , 'Contactmusic.com' , 'Daily Mail']) , ['TITLE' , 'CATEGORY']]\n","\n","train , other = train_test_split(df , test_size=0.2, shuffle=True, random_state=42 , stratify=df['CATEGORY'])\n","valid , test = train_test_split(other , test_size=0.5, shuffle=True, random_state=42 , stratify=other['CATEGORY'])"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"piq55DBKQQ1j","executionInfo":{"status":"ok","timestamp":1648176692879,"user_tz":-540,"elapsed":4,"user":{"displayName":"Tatsuki Okada","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12310138447043973112"}}},"outputs":[],"source":["# ダウンロード制限がかかっているのでprthを指定する\n","# #参考https://qiita.com/jun40vn/items/0f9bd5353197d3f14f3e\n","# ! pip install --upgrade gdown\n","# import gdown\n","# gdown.download('https://drive.google.com/u/0/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download', './GoogleNews-vectors-negative300.bin.gz', quiet=False)\n","# from gensim.models import KeyedVectors\n","# model_kv = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz' , binary=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ymREuxqEIWcx"},"outputs":[],"source":["# 直接pathを指定する場合\n","from gensim.models import KeyedVectors\n","model_kv = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Tutorial/NLP_100/GoogleNews-vectors-negative300.bin.gz' , binary=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0VvqS1hYIuZf"},"outputs":[],"source":["# 特徴ベクトル化\n","import torch\n","import string\n","\n","# 6章と同様の処理\n","def extract(df):\n","  # 記号変換\n","  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n","  words = df.translate(table).split()\n","  # テンソル化\n","  vector = [model_kv[word] for word in words if word in model_kv]\n","\n","  return torch.tensor(sum(vector) / len(vector))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C9zZavsgBG96"},"outputs":[],"source":["X_train = torch.stack([extract(txt) for txt in train['TITLE']])\n","X_valid = torch.stack([extract(txt) for txt in valid['TITLE']])\n","X_test = torch.stack([extract(txt) for txt in test['TITLE']])\n","X_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vI76bqPLKZZc"},"outputs":[],"source":["# ラベル\n","label_map = {'b' : 0 , 't' : 1 , 'e' : 2 , 'm' : 3}\n","y_train = torch.LongTensor(train['CATEGORY'].map(lambda x: label_map[x]).values)\n","y_valid = torch.LongTensor(valid['CATEGORY'].map(lambda x: label_map[x]).values)\n","y_test = torch.LongTensor(test['CATEGORY'].map(lambda x: label_map[x]).values)\n","y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XyB_joXyIuRN"},"outputs":[],"source":["# 参考https://tzmi.hatenablog.com/entry/2020/01/27/001036\n","# https://panda-clip.com/torch-stack/\n","# https://www.hellocybernetics.tech/entry/2017/10/19/070522\n","# https://codezine.jp/article/detail/11052\n","# https://qiita.com/jyori112/items/aad5703c1537c0139edb\n","# https://pytorch.org/docs/stable/generated/torch.save.html"]},{"cell_type":"markdown","metadata":{"id":"hLQupFd_FG7y"},"source":["# 単層ニューラルネットワークによる予測\n","## 問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ．\n","\n","ŷ 1=softmax(x1W),Ŷ =softmax(X[1:4]W)\n","ただし，softmaxはソフトマックス関数，X[1:4]∈ℝ4×dは特徴ベクトルx1,x2,x3,x4を縦に並べた行列である．\n","\n","X[1:4]=⎛⎝⎜⎜⎜⎜x1x2x3x4⎞⎠⎟⎟⎟⎟\n","行列W∈ℝd×Lは単層ニューラルネットワークの重み行列で，ここではランダムな値で初期化すればよい（問題73以降で学習して求める）．なお，ŷ 1∈ℝLは未学習の行列Wで事例x1を分類したときに，各カテゴリに属する確率を表すベクトルである． 同様に，Ŷ ∈ℝn×Lは，学習データの事例x1,x2,x3,x4について，各カテゴリに属する確率を行列として表現している．\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1DM0M_Ude7KM"},"outputs":[],"source":["from torch import nn , optim\n","import torch.nn.functional as F\n","import numpy as np\n","torch.manual_seed(42)\n","\n","X_train = torch.tensor(X_train , requires_grad=True)\n","W = torch.randn(300 , 4)\n","softmax = torch.nn.Softmax(dim=-1)\n","print (f'1 : {softmax(torch.matmul(X_train[:1] , W))}')\n","print (f'4 : {softmax(torch.matmul(X_train[:4] , W))}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdYaPlVqTmlu"},"outputs":[],"source":["# 参考https://qiita.com/mathlive/items/2c67efa2d451ea1da1b1\n","# https://watlab-blog.com/2021/06/13/pytorch-nn-class/"]},{"cell_type":"markdown","metadata":{"id":"qT31R91uFG-w"},"source":["# 損失と勾配の計算\n","## 学習データの事例x1と事例集合x1,x2,x3,x4に対して，クロスエントロピー損失と，行列Wに対する勾配を計算せよ．なお，ある事例xiに対して損失は次式で計算される．\n","\n","li=−log[事例xiがyiに分類される確率]\n","ただし，事例集合に対するクロスエントロピー損失は，その集合に含まれる各事例の損失の平均とする．"]},{"cell_type":"code","source":["class model_ln(nn.Module):\n","  def __init__(self , input_size , output_size):\n","    super().__init__()\n","    self.fc = nn.Linear(input_size , output_size , bias=False)\n","    nn.init.normal_(self.fc.weight , 0.0, 1.0)  # 正規乱数で重みを初期化\n","\n","  def forward(self , x):\n","    x = self.fc(x)\n","    return x"],"metadata":{"id":"wVmC7SbGBxZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2sRgo4Le76a"},"outputs":[],"source":["model = model_ln(300 , 4)\n","CE = nn.CrossEntropyLoss()\n","\n","#_1\n","l_1 = CE(model(X_train[:1]) , y_train[:1])\n","model.zero_grad()\n","l_1.backward()# 誤差算出\n","\n","print(f'クロスエントロピー損失 : {l_1}')\n","print(f'勾配 : {model.fc.weight.grad}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfrIBTEcuIA4"},"outputs":[],"source":["#_4\n","l_4 = CE(model(X_train[:4]) , y_train[:4])\n","model.zero_grad()\n","l_4.backward()\n","\n","print(f'クロスエントロピー損失 : {l_4}')\n","print(f'勾配 : {model.fc.weight.grad}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZO5IFmpjUa2"},"outputs":[],"source":["#参考https://qiita.com/maechanneler/items/8f10a758d7d3431ae61f"]},{"cell_type":"markdown","metadata":{"id":"VQB9mSC0FHBg"},"source":["# 確率的勾配降下法による学習\n","## 確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列Wを学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）．\n","\n"]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","class Creat_dataset(Dataset):\n","  def __init__(self, X, y):\n","    self.X = X\n","    self.y = y\n","\n","  def __len__(self):\n","    return len(self.y)\n","\n","  def __getitem__(self , idx):\n","    return [self.X[idx] , self.y[idx]]"],"metadata":{"id":"Wijq0zPwEcVY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","ds_train = Creat_dataset(X_train , y_train)\n","ds_valid = Creat_dataset(X_valid , y_valid)\n","ds_test = Creat_dataset(X_test , y_test)\n","\n","dataloader_train = DataLoader(ds_train , batch_size=1 , shuffle=True)#混ぜる\n","dataloader_valid = DataLoader(ds_valid , batch_size=len(ds_valid) , shuffle=False)\n","dataloader_test = DataLoader(ds_test , batch_size=len(ds_test) , shuffle=False)"],"metadata":{"id":"D-Ogmxl8EqeH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBS675QJI54t"},"outputs":[],"source":["CE = nn.CrossEntropyLoss()\n","op = optim.SGD(model.parameters() , lr = 1e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJdY9edRAktE"},"outputs":[],"source":["#エポック後\n","for epoch in range(100):\n","    model.train()\n","    loss_train = 0.0\n","    for i, (inputs , labels) in enumerate(dataloader_train):\n","      op.zero_grad()#加算されてしまうのでリセット\n","      y_pred = model(inputs)#modelを用いて予測\n","      loss = CE(y_pred , labels)#誤差\n","      loss.backward()#誤差修正\n","      op.step()#更新\n","      loss_train += loss.item()#誤差\n","\n","    loss_train = loss_train / i#平均誤差\n","    # 検証データ\n","    model.eval() \n","    with torch.no_grad():\n","      inputs , labels = next(iter(dataloader_valid))\n","      outputs = model(inputs)\n","      loss_valid = CE(outputs , labels)\n","\n","    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, loss_valid: {loss_valid:.4f}')                 \n","    # print(f'epoch : {epoch + 1} , loss_train : {loss_train} , loss_valid: {loss_valid}') "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cmRmOUa5cvaQ"},"outputs":[],"source":["# 参考https://qiita.com/mathlive/items/2c67efa2d451ea1da1b1\n","# https://pytorch.org/docs/stable/optim.html"]},{"cell_type":"markdown","metadata":{"id":"m7_iLBR2FHDu"},"source":["# 正解率の計測\n","\n","\n","## 問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ef6ElOqaI485"},"outputs":[],"source":["# from sklearn.metrics import accuracy_score\n","\n","# result_test = model_ln(X_test)\n","# train_score = accuracy_score(y_train / result)\n","# test_score = accuracy_score(y_test / result_test)\n","# print(f'train-score : {train_score}')\n","# print(f'test-score : {test_score}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"su6H2tUrJ3yK"},"outputs":[],"source":["def score(model , loader):\n","  model.eval()\n","  y_true = 0\n","  y_pred = 0\n","  with torch.no_grad():\n","    for inputs , labels in loader:\n","      outputs = model(inputs)\n","      pred = torch.argmax(outputs , dim=-1)\n","      y_true += len(inputs)\n","      y_pred += (pred == labels).sum().item()\n","\n","  return y_pred / y_true"]},{"cell_type":"code","source":["score_train = score(model , dataloader_train)\n","score_test = score(model , dataloader_test)\n","print(f'train ： {score_train}')\n","print(f'valid ： {score_test}')"],"metadata":{"id":"bt_2oUHDLdXG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fqUN_VtKJicB"},"outputs":[],"source":["# 参考https://qiita.com/Haaamaaaaa/items/b9f47cba588b83ad34a7\n","# https://note.nkmk.me/python-pytorch-tensor-item/"]},{"cell_type":"markdown","metadata":{"id":"6r-LVu0EFHGV"},"source":["# 損失と正解率のプロット\n","\n","\n","## 問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1XHbT0hh9xT"},"outputs":[],"source":["def loss_acc(model , criterion , loader):\n","  model.eval()\n","  loss = 0.0\n","  total = 0\n","  correct = 0\n","  with torch.no_grad():\n","    for inputs, labels in loader:\n","      outputs = model(inputs)\n","      loss += criterion(outputs , labels).item()\n","      pred = torch.argmax(outputs , dim=-1)\n","      total += len(inputs)\n","      correct += (pred == labels).sum().item()\n","\n","  return loss / len(loader) , correct / total"]},{"cell_type":"code","source":["# 学習\n","log_train = []\n","log_valid = []\n","for epoch in range(100):\n","  model.train()\n","  for inputs, labels in dataloader_train:\n","    op.zero_grad()#加算されてしまうのでリセット\n","    y_pred = model(inputs)#modelを用いて予測\n","    loss = CE(y_pred , labels)#誤差\n","    loss.backward()#誤差修正\n","    op.step()#更新\n","\n","  # 正解率等の計算\n","  loss_train, acc_train = loss_acc(model , CE , dataloader_train)\n","  loss_valid, acc_valid = loss_acc(model , CE , dataloader_valid)\n","  log_train.append([loss_train , acc_train])\n","  log_valid.append([loss_valid , acc_valid])\n","\n","  # ログを出力\n","  print(f'epoch : {epoch + 1} , loss_train : {loss_train} , accuracy_train : {acc_train} , loss_valid : {loss_valid} , accuracy_valid: {acc_valid}')  \n"],"metadata":{"id":"h_0g3senNQvt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","fig , ax = plt.subplots(1 , 2 , figsize=(25 , 10))\n","#loss_epoch\n","ax[0].plot(np.array(log_train).T[0] , label =' train')\n","ax[0].plot(np.array(log_valid).T[0] , label = 'valid')\n","ax[0].set_xlabel('回数')\n","ax[0].set_ylabel('損失')\n","ax[0].legend()\n","#acc_epoch\n","ax[1].plot(np.array(log_train).T[1], label='train')\n","ax[1].plot(np.array(log_valid).T[1], label='valid')\n","ax[1].set_xlabel('回数')\n","ax[1].set_ylabel('正答率')\n","ax[1].legend()\n","plt.show()"],"metadata":{"id":"VgGuPxPINv0w"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5M3_xPJb6Vz"},"outputs":[],"source":["# https://qiita.com/awawaInu/items/e173acded17a142e6d02"]},{"cell_type":"markdown","metadata":{"id":"Rc0r4dkvFHIm"},"source":["# チェックポイント\n","## 問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ．\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_lkbSK5I3qI"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"PJmQqI3PFHLE"},"source":["# ミニバッチ化\n","## 問題76のコードを改変し，B事例ごとに損失・勾配を計算し，行列Wの値を更新せよ（ミニバッチ化）．Bの値を1,2,4,8,…と変化させながら，1エポックの学習に要する時間を比較せよ．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPnqqX1XI1r6"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"-3DmpIYkFHNo"},"source":["# GPU上での学習\n","## 問題77のコードを改変し，GPU上で学習を実行せよ．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5udvkcHI1I1"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"LonpamKPFHQA"},"source":["# 多層ニューラルネットワーク\n","## 問題78のコードを改変し，バイアス項の導入や多層化など，ニューラルネットワークの形状を変更しながら，高性能なカテゴリ分類器を構築せよ．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_RrLo55OI0Y_"},"outputs":[],"source":[""]}],"metadata":{"colab":{"name":"NLP_100_8.ipynb","provenance":[],"mount_file_id":"1NkwxYqvAel8iC4cI6HIy1Qm1pyqPEP7w","authorship_tag":"ABX9TyMpx2EseyHACBxI9GBJa34P"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}