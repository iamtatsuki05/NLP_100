{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_100_7.ipynb","provenance":[],"authorship_tag":"ABX9TyPMlDYciFvku803DmRy7OYf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 単語ベクトル\n","## 単語の意味を実ベクトルで表現する単語ベクトル（単語埋め込み）に関して，以下の処理を行うプログラムを作成せよ．\n","#### https://nlp100.github.io/ja/ch07.html"],"metadata":{"id":"WhHjR3cWDYRU"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"jBYlYY_Q7o3R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647591426628,"user_tz":-540,"elapsed":23041,"user":{"displayName":"Tatsuki Okada","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12310138447043973112"}},"outputId":"de18bbe6-cac7-46b8-c2f3-a9b65a6f080a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# 単語ベクトルの読み込みと表示\n","## Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル（300万単語・フレーズ，300次元）をダウンロードし，”United States”の単語ベクトルを表示せよ．ただし，”United States”は内部的には”United_States”と表現されていることに注意せよ．"],"metadata":{"id":"jGKwwqzmDYUf"}},{"cell_type":"code","source":[""],"metadata":{"id":"LWa0fAY8EpJe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 単語の類似度\n","## “United States”と”U.S.”のコサイン類似度を計算せよ"],"metadata":{"id":"isrH5f7UDYY3"}},{"cell_type":"code","source":[""],"metadata":{"id":"g8ymJ-k7Epua"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#  類似度の高い単語10件\n","## “United States”とコサイン類似度が高い10語と，その類似度を出力せよ．"],"metadata":{"id":"JOmOugBrDYbz"}},{"cell_type":"code","source":[""],"metadata":{"id":"OuESKMjYEqeJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 加法構成性によるアナロジー\n","## “Spain”の単語ベクトルから”Madrid”のベクトルを引き，”Athens”のベクトルを足したベクトルを計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"],"metadata":{"id":"KVFPai-eDYeO"}},{"cell_type":"code","source":[""],"metadata":{"id":"L4C1v5wQErGb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# アナロジーデータでの実験\n","## 単語アナロジーの評価データをダウンロードし，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．"],"metadata":{"id":"0PdF-CTiDYgz"}},{"cell_type":"code","source":[""],"metadata":{"id":"z63y8DmwEruY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# アナロジータスクでの正解率\n","## 64の実行結果を用い，意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ．"],"metadata":{"id":"sZ-6DovNDYjX"}},{"cell_type":"code","source":[""],"metadata":{"id":"Q-jA5580Esfh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# WordSimilarity-353での評価\n","## The WordSimilarity-353 Test Collectionの評価データをダウンロードし，単語ベクトルにより計算される類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．"],"metadata":{"id":"9Qex_dkADYmC"}},{"cell_type":"code","source":[""],"metadata":{"id":"6pqryGiTEtTs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# k-meansクラスタリング\n","## 国名に関する単語ベクトルを抽出し，k-meansクラスタリングをクラスタ数k=5として実行せよ"],"metadata":{"id":"pEkKFGRiDYog"}},{"cell_type":"code","source":[""],"metadata":{"id":"44yd_hj4EuBq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Ward法によるクラスタリング\n","## 国名に関する単語ベクトルに対し，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．"],"metadata":{"id":"1o5I6j23DYqz"}},{"cell_type":"code","source":[""],"metadata":{"id":"OLcLbKwoEuun"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# t-SNEによる可視化\n","## ベクトル空間上の国名に関する単語ベクトルをt-SNEで可視化せよ"],"metadata":{"id":"h1jVUVdFDYtP"}},{"cell_type":"code","source":[""],"metadata":{"id":"UoozPXpDEvSm"},"execution_count":null,"outputs":[]}]}