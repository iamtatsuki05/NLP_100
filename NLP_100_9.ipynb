{"cells":[{"cell_type":"markdown","metadata":{"id":"wy4FSmYcJIwK"},"source":["# RNN, CNN\n","## https://nlp100.github.io/ja/ch09.html\n"]},{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iamtatsuki05/NLP_100/blob/fix_all_merge/NLP_100_9.ipynb)"],"metadata":{"id":"67ddAau-HU9t"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31117,"status":"ok","timestamp":1649814094944,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"},"user_tz":-540},"id":"jBYlYY_Q7o3R","outputId":"0fd4835d-a42c-44c9-c72d-2dc77e804de1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"CJIjsxfDJcfv"},"source":["# ID番号への変換\n","## 問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6167,"status":"ok","timestamp":1649814101105,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"},"user_tz":-540},"id":"03YyU3nTKzhG","outputId":"a88a50ff-5214-47af-b787-11e1c71199a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-04-13 01:41:34--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n","Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n","Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 29224203 (28M) [application/x-httpd-php]\n","Saving to: ‘NewsAggregatorDataset.zip’\n","\n","NewsAggregatorDatas 100%[===================>]  27.87M  25.3MB/s    in 1.1s    \n","\n","2022-04-13 01:41:36 (25.3 MB/s) - ‘NewsAggregatorDataset.zip’ saved [29224203/29224203]\n","\n","Archive:  NewsAggregatorDataset.zip\n","  inflating: 2pageSessions.csv       \n","   creating: __MACOSX/\n","  inflating: __MACOSX/._2pageSessions.csv  \n","  inflating: newsCorpora.csv         \n","  inflating: __MACOSX/._newsCorpora.csv  \n","  inflating: readme.txt              \n","  inflating: __MACOSX/._readme.txt   \n"]},{"output_type":"execute_result","data":{"text/plain":["{'to': 1,\n"," 's': 2,\n"," 'in': 3,\n"," 'UPDATE': 4,\n"," 'as': 5,\n"," 'of': 6,\n"," 'for': 7,\n"," 'US': 8,\n"," 'The': 9,\n"," 'on': 10,\n"," '1': 11,\n"," 'To': 12,\n"," 'the': 13,\n"," 'and': 14,\n"," '2': 15,\n"," 'at': 16,\n"," 'Of': 17,\n"," 'In': 18,\n"," 'With': 19,\n"," 'a': 20,\n"," 'A': 21,\n"," 'after': 22,\n"," 'For': 23,\n"," 'Is': 24,\n"," 'And': 25,\n"," '3': 26,\n"," 'with': 27,\n"," 'Kardashian': 28,\n"," 'New': 29,\n"," 'by': 30,\n"," 'China': 31,\n"," 'up': 32,\n"," 'After': 33,\n"," 'At': 34,\n"," 'On': 35,\n"," '4': 36,\n"," 'From': 37,\n"," 'Kim': 38,\n"," 'says': 39,\n"," 'is': 40,\n"," 'STOCKS': 41,\n"," 'from': 42,\n"," '5': 43,\n"," 'Wall': 44,\n"," 'Miley': 45,\n"," 'Cyrus': 46,\n"," 'May': 47,\n"," 'her': 48,\n"," 'Fed': 49,\n"," 'Google': 50,\n"," 't': 51,\n"," 'new': 52,\n"," 'CEO': 53,\n"," 'ECB': 54,\n"," 'shares': 55,\n"," 'Euro': 56,\n"," 'Says': 57,\n"," 'West': 58,\n"," 'Billion': 59,\n"," 'How': 60,\n"," 'Chris': 61,\n"," 'data': 62,\n"," 'S': 63,\n"," 'About': 64,\n"," 'Are': 65,\n"," 'More': 66,\n"," 'St': 67,\n"," 'It': 68,\n"," 'First': 69,\n"," 'Over': 70,\n"," 'FOREX': 71,\n"," 'American': 72,\n"," 'it': 73,\n"," 'Bieber': 74,\n"," 'Will': 75,\n"," 'Kanye': 76,\n"," 'bln': 77,\n"," 'Ukraine': 78,\n"," 'Be': 79,\n"," 'Dollar': 80,\n"," 'Justin': 81,\n"," 'Up': 82,\n"," '7': 83,\n"," 'I': 84,\n"," 'You': 85,\n"," 'over': 86,\n"," 'UK': 87,\n"," 'That': 88,\n"," 'Star': 89,\n"," 'Apple': 90,\n"," 'GM': 91,\n"," 'Bank': 92,\n"," 'be': 93,\n"," 'Day': 94,\n"," '2014': 95,\n"," 'Time': 96,\n"," 'euro': 97,\n"," 'has': 98,\n"," 'P': 99,\n"," 'Game': 100,\n"," 'GLOBAL': 101,\n"," 'As': 102,\n"," 'RPT': 103,\n"," '6': 104,\n"," 'off': 105,\n"," 'sales': 106,\n"," 'CORRECTED': 107,\n"," 'By': 108,\n"," 'Jay': 109,\n"," 'Sales': 110,\n"," 'IPO': 111,\n"," 'she': 112,\n"," 'profit': 113,\n"," 'Million': 114,\n"," 'This': 115,\n"," 'Why': 116,\n"," 'first': 117,\n"," 'Beyonce': 118,\n"," 'year': 119,\n"," 'Ford': 120,\n"," 'was': 121,\n"," 'Thrones': 122,\n"," 'out': 123,\n"," 'Z': 124,\n"," 'growth': 125,\n"," 'but': 126,\n"," 'that': 127,\n"," 'Profit': 128,\n"," 'Lindsay': 129,\n"," 'Lohan': 130,\n"," 'Shares': 131,\n"," 'are': 132,\n"," 'can': 133,\n"," 'MARKETS': 134,\n"," 'T': 135,\n"," 'not': 136,\n"," 'No': 137,\n"," 'America': 138,\n"," '10': 139,\n"," 'down': 140,\n"," 'near': 141,\n"," '8': 142,\n"," 'low': 143,\n"," 'What': 144,\n"," 'Selena': 145,\n"," 'Deal': 146,\n"," 'York': 147,\n"," 'deal': 148,\n"," 'Said': 149,\n"," 'Jennifer': 150,\n"," 'Your': 151,\n"," 'Stocks': 152,\n"," 'Second': 153,\n"," 'rises': 154,\n"," 'Brown': 155,\n"," 'Gomez': 156,\n"," 'Twitter': 157,\n"," 'he': 158,\n"," 'Its': 159,\n"," 'Pfizer': 160,\n"," 'high': 161,\n"," 'Out': 162,\n"," 'his': 163,\n"," 'Met': 164,\n"," 'pct': 165,\n"," 'Yellen': 166,\n"," 'We': 167,\n"," 'Before': 168,\n"," 'shows': 169,\n"," 'could': 170,\n"," 'Shows': 171,\n"," 'Pay': 172,\n"," 'Big': 173,\n"," 'But': 174,\n"," 'business': 175,\n"," 'Low': 176,\n"," 'Most': 177,\n"," 'Street': 178,\n"," 'Draghi': 179,\n"," 'World': 180,\n"," 'Alibaba': 181,\n"," 'record': 182,\n"," 'more': 183,\n"," 'than': 184,\n"," 'Wedding': 185,\n"," 'Paul': 186,\n"," 'sees': 187,\n"," 'High': 188,\n"," 'All': 189,\n"," 'Amazon': 190,\n"," 'Amid': 191,\n"," 'Europe': 192,\n"," 'Music': 193,\n"," 'European': 194,\n"," 'Year': 195,\n"," 'Ryan': 196,\n"," 'Her': 197,\n"," 'buy': 198,\n"," 'about': 199,\n"," 'stocks': 200,\n"," 'BNP': 201,\n"," 'Not': 202,\n"," 'Rise': 203,\n"," 'Estimates': 204,\n"," 'Drops': 205,\n"," 'Apparel': 206,\n"," 'Japan': 207,\n"," 'oil': 208,\n"," 'Just': 209,\n"," 'Episode': 210,\n"," 'REFILE': 211,\n"," 'set': 212,\n"," 'rise': 213,\n"," 'Facebook': 214,\n"," 'Off': 215,\n"," 'One': 216,\n"," 'Iraq': 217,\n"," 'Month': 218,\n"," 'Who': 219,\n"," 'TV': 220,\n"," 'Data': 221,\n"," 'month': 222,\n"," 'Have': 223,\n"," 'Emma': 224,\n"," 'pay': 225,\n"," 'AstraZeneca': 226,\n"," 'Awards': 227,\n"," 'Health': 228,\n"," 'fall': 229,\n"," 'before': 230,\n"," 'day': 231,\n"," 'July': 232,\n"," 'Since': 233,\n"," 'Fitch': 234,\n"," 'Netflix': 235,\n"," 'Has': 236,\n"," 'Wars': 237,\n"," 'Mother': 238,\n"," 'James': 239,\n"," 'Internet': 240,\n"," 'flat': 241,\n"," 'Dies': 242,\n"," 'Russia': 243,\n"," 'Economy': 244,\n"," 'ahead': 245,\n"," 'Ebola': 246,\n"," 'She': 247,\n"," 'Was': 248,\n"," 'no': 249,\n"," 'lower': 250,\n"," 'Season': 251,\n"," 'zone': 252,\n"," 'Still': 253,\n"," 'Love': 254,\n"," 'Cancer': 255,\n"," 'billion': 256,\n"," 'jobs': 257,\n"," 'Mobile': 258,\n"," 'German': 259,\n"," 'court': 260,\n"," 'Jenner': 261,\n"," 'reveals': 262,\n"," 'will': 263,\n"," 'Outlook': 264,\n"," 'Record': 265,\n"," 'Hong': 266,\n"," 'Kong': 267,\n"," '100': 268,\n"," 'talks': 269,\n"," 'Michael': 270,\n"," 'Show': 271,\n"," 'Drop': 272,\n"," 'prices': 273,\n"," 'Growth': 274,\n"," 'revenue': 275,\n"," 'gets': 276,\n"," 'Get': 277,\n"," 'Harrison': 278,\n"," 'Walker': 279,\n"," 'Video': 280,\n"," 'falls': 281,\n"," 'Gold': 282,\n"," 'have': 283,\n"," 'June': 284,\n"," 'Dead': 285,\n"," 'Samsung': 286,\n"," 'Down': 287,\n"," 'million': 288,\n"," 'Film': 289,\n"," 'Know': 290,\n"," 'Jackson': 291,\n"," 'So': 292,\n"," 'Talks': 293,\n"," 'Box': 294,\n"," 'FDA': 295,\n"," 'Men': 296,\n"," 'Cars': 297,\n"," 'Falls': 298,\n"," 'two': 299,\n"," '500': 300,\n"," 'end': 301,\n"," 'Makes': 302,\n"," 'Chinese': 303,\n"," 'BOJ': 304,\n"," 'Movie': 305,\n"," 'Fox': 306,\n"," 'world': 307,\n"," 'higher': 308,\n"," 'man': 309,\n"," 'during': 310,\n"," 'Robin': 311,\n"," 'Thicke': 312,\n"," 'Into': 313,\n"," 'GRAINS': 314,\n"," 'into': 315,\n"," 'takes': 316,\n"," 'Two': 317,\n"," 'Home': 318,\n"," 'source': 319,\n"," 'maker': 320,\n"," 'Times': 321,\n"," 'who': 322,\n"," 'Khloe': 323,\n"," 'claims': 324,\n"," 'against': 325,\n"," 'inflation': 326,\n"," 'Biggest': 327,\n"," 'Credit': 328,\n"," 'drug': 329,\n"," 'King': 330,\n"," 'Futures': 331,\n"," 'economy': 332,\n"," 'Stars': 333,\n"," 'Brad': 334,\n"," 'hit': 335,\n"," 'wedding': 336,\n"," 'Obamacare': 337,\n"," 'Some': 338,\n"," 'India': 339,\n"," 'His': 340,\n"," 'home': 341,\n"," 'Now': 342,\n"," 'Captain': 343,\n"," 'top': 344,\n"," 'L': 345,\n"," 'Scott': 346,\n"," 'Climate': 347,\n"," 'BofA': 348,\n"," 'seen': 349,\n"," 'they': 350,\n"," 'Play': 351,\n"," 'SNAPSHOT': 352,\n"," 'rates': 353,\n"," 'Argentina': 354,\n"," 'rate': 355,\n"," 'Voice': 356,\n"," 'J': 357,\n"," 'policy': 358,\n"," 'Week': 359,\n"," 'cancer': 360,\n"," 'years': 361,\n"," 'show': 362,\n"," 'Live': 363,\n"," 'Stone': 364,\n"," 'Tesla': 365,\n"," 'McDonald': 366,\n"," 'Alstom': 367,\n"," 'offer': 368,\n"," 'gains': 369,\n"," 'Mortgage': 370,\n"," 'Transformers': 371,\n"," '9': 372,\n"," 'may': 373,\n"," 'Carney': 374,\n"," 'U': 375,\n"," 'Five': 376,\n"," 'its': 377,\n"," 'Buy': 378,\n"," 'Market': 379,\n"," 'Gain': 380,\n"," 'Birthday': 381,\n"," 'Comcast': 382,\n"," 'Seth': 383,\n"," 'take': 384,\n"," 'Trailer': 385,\n"," 'steady': 386,\n"," 'market': 387,\n"," 'IMF': 388,\n"," 'UN': 389,\n"," 'Gwyneth': 390,\n"," 'Paltrow': 391,\n"," 'week': 392,\n"," 'Mick': 393,\n"," 'Forecast': 394,\n"," 'help': 395,\n"," 'WRAPUP': 396,\n"," 'bond': 397,\n"," 'Lea': 398,\n"," 'Michele': 399,\n"," 'Pregnant': 400,\n"," 'Beats': 401,\n"," 'Disney': 402,\n"," 'Easter': 403,\n"," 'Christina': 404,\n"," 'an': 405,\n"," 'MERS': 406,\n"," 'Hollywood': 407,\n"," 'back': 408,\n"," 'PRECIOUS': 409,\n"," 'Suit': 410,\n"," 'French': 411,\n"," 'Rolling': 412,\n"," 'Won': 413,\n"," 'Years': 414,\n"," 'raises': 415,\n"," 'House': 416,\n"," 'plan': 417,\n"," 'Warner': 418,\n"," 'George': 419,\n"," 'forecast': 420,\n"," 'Martin': 421,\n"," 'Case': 422,\n"," 'FCC': 423,\n"," 'Can': 424,\n"," 'Affirms': 425,\n"," 'April': 426,\n"," 'Oil': 427,\n"," 'Jagger': 428,\n"," 'Settlement': 429,\n"," 'Young': 430,\n"," 'makes': 431,\n"," 'Could': 432,\n"," 'people': 433,\n"," 'VIDEO': 434,\n"," 'Set': 435,\n"," 'ends': 436,\n"," 'cuts': 437,\n"," 'next': 438,\n"," 'hike': 439,\n"," 'Taylor': 440,\n"," 'HBO': 441,\n"," 'service': 442,\n"," 'holds': 443,\n"," 'plans': 444,\n"," 'Life': 445,\n"," 'Yahoo': 446,\n"," 'Rises': 447,\n"," 'gas': 448,\n"," 'all': 449,\n"," 'Air': 450,\n"," 'you': 451,\n"," 'Corn': 452,\n"," 'Australia': 453,\n"," 'Review': 454,\n"," 'slips': 455,\n"," 'South': 456,\n"," 'hits': 457,\n"," 'Stones': 458,\n"," 'EU': 459,\n"," 'GDP': 460,\n"," 'concerns': 461,\n"," 'Gosling': 462,\n"," 'Africa': 463,\n"," 'Williams': 464,\n"," 'Bid': 465,\n"," 'Ban': 466,\n"," 'Lopez': 467,\n"," 'their': 468,\n"," 'Barclays': 469,\n"," 'Mila': 470,\n"," 'Kunis': 471,\n"," 'Director': 472,\n"," 'Kate': 473,\n"," 'News': 474,\n"," 'David': 475,\n"," 'sale': 476,\n"," 'firm': 477,\n"," 'Festival': 478,\n"," 'AP': 479,\n"," '—': 480,\n"," 'face': 481,\n"," 'France': 482,\n"," 'warns': 483,\n"," 'Probe': 484,\n"," 'Role': 485,\n"," 'study': 486,\n"," 'company': 487,\n"," 'Go': 488,\n"," 'End': 489,\n"," 'Look': 490,\n"," 'Red': 491,\n"," 'start': 492,\n"," 'Against': 493,\n"," 'expected': 494,\n"," 'posts': 495,\n"," 'Spider': 496,\n"," 'Raises': 497,\n"," 'quarter': 498,\n"," 'Sees': 499,\n"," 'mln': 500,\n"," 'Gets': 501,\n"," 'An': 502,\n"," 'Deutsche': 503,\n"," 'Americans': 504,\n"," 'earnings': 505,\n"," 'Best': 506,\n"," 'Gary': 507,\n"," 'People': 508,\n"," 'Citigroup': 509,\n"," 'British': 510,\n"," 'NY': 511,\n"," 'Johnny': 512,\n"," 'Depp': 513,\n"," 'fight': 514,\n"," 'John': 515,\n"," 'Top': 516,\n"," 'Morgan': 517,\n"," 'Brent': 518,\n"," 'Open': 519,\n"," 'second': 520,\n"," 'recall': 521,\n"," 'Real': 522,\n"," 'Noah': 523,\n"," 'Opens': 524,\n"," 'Swift': 525,\n"," 'premiere': 526,\n"," 'banks': 527,\n"," 'still': 528,\n"," 'use': 529,\n"," 'Lena': 530,\n"," 'Dunham': 531,\n"," 'ex': 532,\n"," 'work': 533,\n"," 'Andrew': 534,\n"," 'Garfield': 535,\n"," 'loss': 536,\n"," 'Women': 537,\n"," 'this': 538,\n"," 'Rate': 539,\n"," 'Than': 540,\n"," 'Premiere': 541,\n"," 'sell': 542,\n"," 'GE': 543,\n"," 'Bad': 544,\n"," 'report': 545,\n"," 'test': 546,\n"," 'Share': 547,\n"," 'since': 548,\n"," 'Streaming': 549,\n"," 'Beyoncé': 550,\n"," 'MTV': 551,\n"," 'Girls': 552,\n"," 'Reveals': 553,\n"," 'Stable': 554,\n"," 'while': 555,\n"," 'Here': 556,\n"," 'List': 557,\n"," 'bid': 558,\n"," 'Jimmy': 559,\n"," 'return': 560,\n"," 'drops': 561,\n"," 'Glass': 562,\n"," 'Don': 563,\n"," 'Paribas': 564,\n"," 'Near': 565,\n"," 'Lily': 566,\n"," 'White': 567,\n"," 'He': 568,\n"," 'away': 569,\n"," 'beats': 570,\n"," 'Shire': 571,\n"," 'results': 572,\n"," 'Solange': 573,\n"," 'Shia': 574,\n"," 'AT': 575,\n"," 'Russell': 576,\n"," 'key': 577,\n"," 'Cannes': 578,\n"," 'Rally': 579,\n"," 'Tax': 580,\n"," 'party': 581,\n"," 'Man': 582,\n"," 'Better': 583,\n"," 'Three': 584,\n"," 'Lana': 585,\n"," 'Del': 586,\n"," 'Rey': 587,\n"," 'VII': 588,\n"," 'now': 589,\n"," 'Jump': 590,\n"," 'Office': 591,\n"," 'Robert': 592,\n"," 'Baby': 593,\n"," 'Canada': 594,\n"," 'red': 595,\n"," 'Report': 596,\n"," 'Recall': 597,\n"," 'Court': 598,\n"," '11': 599,\n"," 'percent': 600,\n"," 'last': 601,\n"," 'Allen': 602,\n"," 'Angelina': 603,\n"," 'Jolie': 604,\n"," 'Again': 605,\n"," 'hospital': 606,\n"," 'official': 607,\n"," 'Fight': 608,\n"," 'Inflation': 609,\n"," 'London': 610,\n"," 'highs': 611,\n"," 'trading': 612,\n"," 'Bell': 613,\n"," 'open': 614,\n"," 'Do': 615,\n"," 'Stanley': 616,\n"," 'box': 617,\n"," 'Action': 618,\n"," 'Senate': 619,\n"," 'Really': 620,\n"," 'DirecTV': 621,\n"," 'film': 622,\n"," 'debt': 623,\n"," 'again': 624,\n"," 'Jessica': 625,\n"," 'Last': 626,\n"," 'Obama': 627,\n"," 'Takes': 628,\n"," 'NOT': 629,\n"," 'hopes': 630,\n"," 'Asia': 631,\n"," 'NASA': 632,\n"," 'IBM': 633,\n"," 'video': 634,\n"," 'made': 635,\n"," 'Black': 636,\n"," 'eyes': 637,\n"," 'virus': 638,\n"," 'one': 639,\n"," 'Coachella': 640,\n"," 'Make': 641,\n"," 'strike': 642,\n"," 'My': 643,\n"," 'Did': 644,\n"," 'See': 645,\n"," 'BlackBerry': 646,\n"," 'Pushes': 647,\n"," '14': 648,\n"," 'heart': 649,\n"," 'app': 650,\n"," 'Photo': 651,\n"," 'Climbs': 652,\n"," 'iPad': 653,\n"," 'BOE': 654,\n"," 'Let': 655,\n"," 'Goldman': 656,\n"," 'Bryan': 657,\n"," 'Singer': 658,\n"," 'ever': 659,\n"," 'Sex': 660,\n"," 'Heart': 661,\n"," 'Sprint': 662,\n"," 'close': 663,\n"," 'Sell': 664,\n"," '12': 665,\n"," 'drop': 666,\n"," 'Clooney': 667,\n"," 'Summer': 668,\n"," 'Stimulus': 669,\n"," 'wins': 670,\n"," '17': 671,\n"," 'Suisse': 672,\n"," 'Gay': 673,\n"," 'cars': 674,\n"," 'Banks': 675,\n"," 'comments': 676,\n"," 'Rob': 677,\n"," 'Hemsworth': 678,\n"," 'Instagram': 679,\n"," 'Recap': 680,\n"," 'Rogen': 681,\n"," 'case': 682,\n"," 'time': 683,\n"," '50': 684,\n"," 'Or': 685,\n"," 'Would': 686,\n"," 'wife': 687,\n"," 'Earth': 688,\n"," 'weak': 689,\n"," 'Detroit': 690,\n"," 'Japanese': 691,\n"," 'Mars': 692,\n"," 'Calls': 693,\n"," 'Weekend': 694,\n"," 'Nasdaq': 695,\n"," 'Pitt': 696,\n"," 'bank': 697,\n"," 'Free': 698,\n"," 'if': 699,\n"," 'Nick': 700,\n"," 'days': 701,\n"," '22': 702,\n"," 'Cover': 703,\n"," 'Reaches': 704,\n"," 'Prices': 705,\n"," 'just': 706,\n"," 'board': 707,\n"," 'E': 708,\n"," 'trade': 709,\n"," 'Toyota': 710,\n"," 'Shailene': 711,\n"," 'Woodley': 712,\n"," 'Other': 713,\n"," 'Fall': 714,\n"," 'Target': 715,\n"," 'Another': 716,\n"," 'Tour': 717,\n"," 'Back': 718,\n"," 'Everything': 719,\n"," 'cut': 720,\n"," 'Gains': 721,\n"," 'Gala': 722,\n"," 'debut': 723,\n"," 'North': 724,\n"," 'light': 725,\n"," 'six': 726,\n"," 'Kristen': 727,\n"," 'Peaches': 728,\n"," 'Geldof': 729,\n"," 'Funeral': 730,\n"," 'Winter': 731,\n"," 'BMW': 732,\n"," 'Tribute': 733,\n"," 'Wren': 734,\n"," 'Misses': 735,\n"," 'Foods': 736,\n"," 'Slump': 737,\n"," 'panel': 738,\n"," 'Queen': 739,\n"," 'Johnson': 740,\n"," 'opens': 741,\n"," 'Demi': 742,\n"," 'Lovato': 743,\n"," 'Breaks': 744,\n"," 'crash': 745,\n"," 'Virus': 746,\n"," 'Challenge': 747,\n"," 'JPMorgan': 748,\n"," 'Jon': 749,\n"," 'Night': 750,\n"," '13': 751,\n"," 'Support': 752,\n"," 'Cut': 753,\n"," 'supply': 754,\n"," 'need': 755,\n"," 'Watch': 756,\n"," 'Cases': 757,\n"," 'outlook': 758,\n"," 'Joe': 759,\n"," 'Drew': 760,\n"," 'goes': 761,\n"," 'energy': 762,\n"," 'Dispute': 763,\n"," 'most': 764,\n"," 'leaves': 765,\n"," 'Drug': 766,\n"," 'Death': 767,\n"," 'Wanted': 768,\n"," 'Allergan': 769,\n"," 'March': 770,\n"," 'BET': 771,\n"," 'fund': 772,\n"," 'Meet': 773,\n"," 'Good': 774,\n"," 'arrives': 775,\n"," 'gown': 776,\n"," 'see': 777,\n"," 'so': 778,\n"," 'Rates': 779,\n"," 'Nicki': 780,\n"," 'Minaj': 781,\n"," 'Vogue': 782,\n"," 'Even': 783,\n"," 'confirms': 784,\n"," 'Wins': 785,\n"," 'Rooney': 786,\n"," 'order': 787,\n"," 'probe': 788,\n"," 'Kendall': 789,\n"," 'bonds': 790,\n"," 'merger': 791,\n"," 'Net': 792,\n"," 'Neutrality': 793,\n"," 'Battle': 794,\n"," '24': 795,\n"," 'Superman': 796,\n"," 'tour': 797,\n"," 'Celebrates': 798,\n"," 'NEW': 799,\n"," 'YORK': 800,\n"," 'being': 801,\n"," 'Lady': 802,\n"," 'Gaga': 803,\n"," 'AA': 804,\n"," 'Chrysler': 805,\n"," 'does': 806,\n"," 'Sanctions': 807,\n"," 'estimates': 808,\n"," 'Tv': 809,\n"," 'Friends': 810,\n"," 'LaBeouf': 811,\n"," 'Whole': 812,\n"," 'signs': 813,\n"," 'Finale': 814,\n"," 'Biopic': 815,\n"," 'Never': 816,\n"," 'Dee': 817,\n"," 'Bill': 818,\n"," 'ring': 819,\n"," 'Actor': 820,\n"," 'WSJ': 821,\n"," 'Ex': 822,\n"," 'Business': 823,\n"," 'Study': 824,\n"," 'manufacturing': 825,\n"," 'small': 826,\n"," 'Investors': 827,\n"," 'Bonds': 828,\n"," 'Fans': 829,\n"," 'Tech': 830,\n"," 'Amazing': 831,\n"," 'Move': 832,\n"," 'Family': 833,\n"," 'Like': 834,\n"," 'Workers': 835,\n"," 'Things': 836,\n"," 'Change': 837,\n"," 'costs': 838,\n"," '16': 839,\n"," 'Close': 840,\n"," 'boost': 841,\n"," 'global': 842,\n"," 'edge': 843,\n"," 'Daniel': 844,\n"," 'Kimye': 845,\n"," 'jump': 846,\n"," 'son': 847,\n"," 'Called': 848,\n"," 'Oldman': 849,\n"," 'investigation': 850,\n"," 'Plane': 851,\n"," 'Kylie': 852,\n"," 'Anti': 853,\n"," 'Aussie': 854,\n"," 'name': 855,\n"," 'Divergent': 856,\n"," 'Mad': 857,\n"," 'Cable': 858,\n"," 'Risk': 859,\n"," 'Tom': 860,\n"," 'half': 861,\n"," 'Peter': 862,\n"," 'Texas': 863,\n"," 'Washington': 864,\n"," 'call': 865,\n"," 'Little': 866,\n"," 'investment': 867,\n"," 'Six': 868,\n"," 'missing': 869,\n"," 'Story': 870,\n"," 'Franco': 871,\n"," 'Leads': 872,\n"," 'Beat': 873,\n"," 'Single': 874,\n"," 'Musk': 875,\n"," 'Happy': 876,\n"," 'yields': 877,\n"," 'Q2': 878,\n"," 'Book': 879,\n"," 'carpet': 880,\n"," 'Candy': 881,\n"," 'Crush': 882,\n"," 'trial': 883,\n"," 'car': 884,\n"," 'Zac': 885,\n"," 'Efron': 886,\n"," 'guilty': 887,\n"," 'Guinea': 888,\n"," 'outbreak': 889,\n"," 'Pharrell': 890,\n"," 'wants': 891,\n"," 'Soldier': 892,\n"," 'office': 893,\n"," 'Plan': 894,\n"," 'Ousted': 895,\n"," 'vehicles': 896,\n"," 'sources': 897,\n"," 'Officials': 898,\n"," 'sister': 899,\n"," 'Los': 900,\n"," 'Angeles': 901,\n"," 'little': 902,\n"," 'moves': 903,\n"," 'long': 904,\n"," 'Scientists': 905,\n"," 'Dow': 906,\n"," 'Frozen': 907,\n"," 'Elsa': 908,\n"," 'Once': 909,\n"," 'Upon': 910,\n"," 're': 911,\n"," 'President': 912,\n"," 'McCarthy': 913,\n"," 'Records': 914,\n"," 'Fast': 915,\n"," 'ECONOMY': 916,\n"," 'Heard': 917,\n"," 'C': 918,\n"," 'They': 919,\n"," 'Asian': 920,\n"," 'stimulus': 921,\n"," 'boosts': 922,\n"," 'Idol': 923,\n"," 'Batman': 924,\n"," 'Critics': 925,\n"," 'Their': 926,\n"," 'Say': 927,\n"," 'com': 928,\n"," 'Pregnancy': 929,\n"," 'Father': 930,\n"," 'using': 931,\n"," 'capital': 932,\n"," 'Space': 933,\n"," 'dip': 934,\n"," 'Lot': 935,\n"," 'products': 936,\n"," 've': 937,\n"," 'spread': 938,\n"," 'another': 939,\n"," 'meeting': 940,\n"," 'supplies': 941,\n"," 'Malaysia': 942,\n"," 'Jet': 943,\n"," 'stars': 944,\n"," 'sells': 945,\n"," 'chief': 946,\n"," 'Very': 947,\n"," 'sanctions': 948,\n"," '20': 949,\n"," 'Award': 950,\n"," 'Media': 951,\n"," 'Tattoo': 952,\n"," 'Valeant': 953,\n"," 'Daughter': 954,\n"," 'Michigan': 955,\n"," 'Index': 956,\n"," 'launches': 957,\n"," 'daughter': 958,\n"," 'Bulgaria': 959,\n"," 'BBB': 960,\n"," 'adds': 961,\n"," 'how': 962,\n"," 'Rose': 963,\n"," 'Seen': 964,\n"," 'mother': 965,\n"," 'mortgage': 966,\n"," 'Khloé': 967,\n"," 'Finally': 968,\n"," 'my': 969,\n"," 'live': 970,\n"," 'Bachelorette': 971,\n"," 'Hill': 972,\n"," 'find': 973,\n"," 'BoE': 974,\n"," 'even': 975,\n"," 'yen': 976,\n"," 'news': 977,\n"," 'Phone': 978,\n"," 'expectations': 979,\n"," 'book': 980,\n"," 'fire': 981,\n"," 'Gives': 982,\n"," 'Half': 983,\n"," 'Plant': 984,\n"," 'puts': 985,\n"," 'dies': 986,\n"," 'Products': 987,\n"," 'Coming': 988,\n"," 'Keith': 989,\n"," 'him': 990,\n"," 'Taking': 991,\n"," 'Yuan': 992,\n"," 'Weekly': 993,\n"," 'Seek': 994,\n"," 'Claims': 995,\n"," 'Hotel': 996,\n"," 'Too': 997,\n"," 'Former': 998,\n"," 'Head': 999,\n"," 'Sorry': 1000,\n"," ...}"]},"metadata":{},"execution_count":2}],"source":["#50,51をもう一度\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n","!unzip NewsAggregatorDataset.zip\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('/content/newsCorpora.csv' , header=None , sep='\\t' , names=['ID' , 'TITLE' , 'URL' , 'PUBLISHER' , 'CATEGORY' , 'STORY' , 'HOSTNAME' , 'TIMESTAMP'])\n","\n","# データの抽出\n","df = df.loc[df['PUBLISHER'].isin(['Reuters' , 'Huffington Post' , 'Businessweek' , 'Contactmusic.com' , 'Daily Mail']) , ['TITLE' , 'CATEGORY']]\n","\n","# データの分割\n","test , train_valid = train_test_split(df , test_size = 0.8 , shuffle = True , random_state = 42 , stratify = df['CATEGORY'])\n","valid , train = train_test_split(train_valid , test_size = 0.25 , shuffle = True , random_state = 42 , stratify = train_valid['CATEGORY'])\n","train.reset_index(drop = True , inplace = True)\n","valid.reset_index(drop = True , inplace = True)\n","test.reset_index(drop = True , inplace = True)\n","\n","from collections import defaultdict\n","import string\n","\n","d = defaultdict(int)\n","table = str.maketrans(string.punctuation , ' '*len(string.punctuation))#記号処理\n","for text in train['TITLE']:\n","    for word in text.translate(table).split():\n","        d[word] += 1\n","d = sorted(d.items() , key=lambda x:x[1] , reverse=True)#sort\n","\n","word_id = {word: idx + 1 for idx , (word , num) in enumerate(d) if num > 1}#辞書\n","word_id"]},{"cell_type":"code","source":["PAD_TOKEN = '<PAD>'\n","UNK_TOKEN = '<UNK>'\n","PAD = 0 \n","UNK = 1 "],"metadata":{"id":"xyOkRJSIcAxp","executionInfo":{"status":"ok","timestamp":1649814101105,"user_tz":-540,"elapsed":11,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["word2id = {\n","    PAD_TOKEN: PAD,\n","    UNK_TOKEN: UNK,\n","}\n","\n","MIN_COUNT = 1"],"metadata":{"id":"ozN7-AMWcCkv","executionInfo":{"status":"ok","timestamp":1649814101106,"user_tz":-540,"elapsed":11,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class Vocab(object):\n","\n","    def __init__(self, word2id={}):\n","        self.word2id = dict(word2id)\n","        self.id2word = {v: k for k , v in self.word2id.items()}    \n","\n","    def build_vocab(self , sentences , min_count = 1):\n","        word_counter = {}\n","        for sentence in sentences:\n","            for word in sentence:\n","\n","                word_counter[word] = word_counter.get(word , 0) + 1\n","        for word, count in sorted(word_counter.items() , key = lambda x: -x[1]):\n","            if count < min_count:\n","                break\n","            _id = len(self.word2id)\n","            self.word2id.setdefault(word , _id)\n","            self.id2word[_id] = word\n","        self.raw_vocab = {w: word_counter[w] for w in self.word2id.keys() if w in word_counter}"],"metadata":{"id":"YOGqeRz6cKgB","executionInfo":{"status":"ok","timestamp":1649814101106,"user_tz":-540,"elapsed":11,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["vocab = Vocab(word2id = word2id)\n","vocab.build_vocab(train  , min_count = MIN_COUNT)"],"metadata":{"id":"0A5byM-PcTfm","executionInfo":{"status":"ok","timestamp":1649814101106,"user_tz":-540,"elapsed":9,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def sentence_to_ids(vocab, sen):\n","    out = [vocab.word2id.get(word , UNK) for word in sen]\n","    return out"],"metadata":{"id":"hbZjB-bZcVTh","executionInfo":{"status":"ok","timestamp":1649814101107,"user_tz":-540,"elapsed":10,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["id_train = [sentence_to_ids(vocab , sen) for sen in train]\n","print(id_train[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5EZMqwZzcY8O","executionInfo":{"status":"ok","timestamp":1649814101107,"user_tz":-540,"elapsed":10,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}},"outputId":"2cf1dc00-e90a-4a1f-bbfc-7534967ff33a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 4, 2, 5, 3]\n"]}]},{"cell_type":"code","source":["vocab.build_vocab(valid , min_count=MIN_COUNT)\n","id_valid = [sentence_to_ids(vocab , sen) for sen in valid]\n","vocab.build_vocab(test , min_count=MIN_COUNT)\n","id_test = [sentence_to_ids(vocab , sen) for sen in test]"],"metadata":{"id":"5xPslxxUdpj9","executionInfo":{"status":"ok","timestamp":1649814101107,"user_tz":-540,"elapsed":7,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# from collections import Counter\n","\n","# counter = Counter([\n","#     x\n","#     for sent in train\n","#     for x in sent\n","# ])\n","# vocab_in_train = [\n","#     token\n","#     for token, freq in counter.most_common()\n","#     if freq > 1\n","# ]\n","# vocab_list = ['[UNK]'] + vocab_in_train\n","# vocab_dict = {x:n for n , x in enumerate(vocab_list)}"],"metadata":{"id":"LuJtaSjy4j8Q","executionInfo":{"status":"ok","timestamp":1649814101108,"user_tz":-540,"elapsed":8,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# def sent_to_ids(sent):\n","#     return torch.tensor([vocab_dict[x if x in vocab_dict else '[UNK]'] for x in sent] , dtype=torch.long)\n","# def dataset_to_ids(dataset):\n","#     return [sent_to_ids(x) for x in dataset]"],"metadata":{"id":"4T0JM2tp5eWx","executionInfo":{"status":"ok","timestamp":1649814101108,"user_tz":-540,"elapsed":8,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# import torch\n","# train_ds = dataset_to_ids(train)\n","# valid_ds = dataset_to_ids(valid)\n","# test_ds = dataset_to_ids(test)\n","# train_ds[:3]"],"metadata":{"id":"VRFPOGPC5lTV","executionInfo":{"status":"ok","timestamp":1649814101108,"user_tz":-540,"elapsed":7,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RUHR6GfrJcia"},"source":["# RNNによる予測\n","## ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列xからカテゴリyを予測するモデルとして，次式を実装せよ．\n","\n","h→0=0,h→t=RNN−→−−(emb(xt),h→t−1),y=softmax(W(yh)h→T+b(y))\n","ただし，emb(x)∈ℝdwは単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数），h→t∈ℝdhは時刻tの隠れ状態ベクトル，RNN−→−−(x,h)は入力xと前時刻の隠れ状態hから次状態を計算するRNNユニット，W(yh)∈ℝL×dhは隠れ状態ベクトルからカテゴリを予測するための行列，b(y)∈ℝLはバイアス項である（dw,dh,Lはそれぞれ，単語埋め込みの次元数，隠れ状態ベクトルの次元数，ラベル数である）．RNNユニットRNN−→−−(x,h)には様々な構成が考えられるが，典型例として次式が挙げられる．\n","\n","RNN−→−−(x,h)=g(W(hx)x+W(hh)h+b(h))\n","ただし，W(hx)∈ℝdh×dw，W(hh)∈ℝdh×dh,b(h)∈ℝdhはRNNユニットのパラメータ，gは活性化関数（例えばtanhやReLUなど）である．\n","\n","なお，この問題ではパラメータの学習を行わず，ランダムに初期化されたパラメータでyを計算するだけでよい．次元数などのハイパーパラメータは，dw=300,dh=50など，適当な値に設定せよ（以降の問題でも同様である）．"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"HZAWporJ92Nc","executionInfo":{"status":"ok","timestamp":1649814101109,"user_tz":-540,"elapsed":8,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["#dw=300,dh=50"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"cmQohg-guRUT","executionInfo":{"status":"ok","timestamp":1649814106560,"user_tz":-540,"elapsed":5459,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","torch.manual_seed(42)\n","\n","class RNN(nn.Module):\n","    def __init__(self , vocab_size , emb_size , padding_idx , output_size , hidden_size):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.emb = nn.Embedding(vocab_size , emb_size , padding_idx=padding_idx)\n","        self.rnn = nn.RNN(emb_size , hidden_size , nonlinearity='relu' , batch_first=True)\n","        self.fc = nn.Linear(hidden_size , output_size)\n","        \n","    def forward(self, x):\n","        self.batch_size = x.size()[0]\n","        hidden = self.init_hidden()\n","        emb = self.emb(x)\n","        out, hidden = self.rnn(emb , hidden)\n","        out = self.fc(out[: , -1 , :])\n","        return out\n","        \n","    def init_hidden(self):\n","        hidden = torch.zeros(1, self.batch_size , self.hidden_size)\n","        return hidden"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"_YJxuPDht3Zv","executionInfo":{"status":"ok","timestamp":1649814106797,"user_tz":-540,"elapsed":250,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["import torch\n","#テキスト修正\n","def tokenizer(text , word2id=word_id , unk = 0):\n","    table = str.maketrans(string.punctuation , ' '*len(string.punctuation))\n","    return [word2id.get(word , unk) for word in text.translate(table).split()]"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"UDZqdi2OuUjb","executionInfo":{"status":"ok","timestamp":1649814106798,"user_tz":-540,"elapsed":4,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class NewsCorporaDataset(Dataset):\n","    def __init__(self , X , y , tokenizer):\n","        self.X = X\n","        self.y = y\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.y)\n","\n","    def __getitem__(self , index):\n","        text = self.X[index]\n","        inputs = self.tokenizer(text)\n","\n","        return {'inputs': torch.tensor(inputs , dtype=torch.int64) , 'labels': torch.tensor(self.y[index] , dtype=torch.int64)}"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649814106798,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"},"user_tz":-540},"id":"EaTS841ruWCD","outputId":"c139e91c-1124-4384-f706-23ae80949945"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1838, 0.2336, 0.0934, 0.4892]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.0377, 0.2589, 0.1205, 0.5828]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.0550, 0.1894, 0.0484, 0.7072]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2872, 0.2067, 0.3378, 0.1683]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2435, 0.3364, 0.2199, 0.2002]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.1315, 0.4774, 0.0772, 0.3139]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.1498, 0.2142, 0.1519, 0.4841]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2632, 0.2574, 0.2965, 0.1830]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2359, 0.3114, 0.1938, 0.2589]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.1960, 0.3173, 0.1895, 0.2972]], grad_fn=<SoftmaxBackward0>)\n"]}],"source":["# ラベルベクトル\n","category_dict = {'b': 0 , 't': 1 , 'e':2 , 'm':3}\n","y_train = train['CATEGORY'].map(lambda x: category_dict[x]).values\n","y_valid = valid['CATEGORY'].map(lambda x: category_dict[x]).values\n","y_test = test['CATEGORY'].map(lambda x: category_dict[x]).values\n","\n","dataset_train = NewsCorporaDataset(train['TITLE'] , y_train , tokenizer)\n","dataset_valid = NewsCorporaDataset(valid['TITLE'] , y_valid , tokenizer)\n","dataset_test = NewsCorporaDataset(test['TITLE'] , y_test , tokenizer)\n","\n","vocab_size = len(set(word_id.values())) + 1\n","emb_size = 300\n","padding_idx = len(set(word_id.values()))\n","output_size = 4\n","hidden_size = 50\n","\n","model = RNN(vocab_size , emb_size , padding_idx , output_size , hidden_size)\n","\n","# 先頭10件の予測値取得\n","for num in range(10):\n","  X = dataset_train[num]['inputs']\n","  print(torch.softmax(model(X.unsqueeze(0)) , dim=-1))"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"9xFNbtAhKyyt","executionInfo":{"status":"ok","timestamp":1649814107067,"user_tz":-540,"elapsed":271,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["# 参考https://exture-ri.com/2021/01/12/pytorch-rnn/\n","# https://gotutiyan.hatenablog.com/entry/2020/09/02/200144"]},{"cell_type":"markdown","metadata":{"id":"Ybr_R3_0Jckm"},"source":["# 確率的勾配降下法による学習\n","## 確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"7Plx2-1lKyPm","executionInfo":{"status":"ok","timestamp":1649814107068,"user_tz":-540,"elapsed":5,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from torch import optim\n","\n","def calc_acc(model , dataset , device =None , criterion = None):\n","    dataloader = DataLoader(dataset , batch_size = 1 , shuffle=False)\n","    loss = 0.0\n","    total = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data in dataloader:\n","        \n","            inputs = data['inputs'].to(device)#gpu\n","            labels = data['labels'].to(device)#gpu\n","            outputs = model(inputs)#計算\n","            if criterion != None:\n","                loss += criterion(outputs, labels).item()#loss\n","                #正解率\n","            pred = torch.argmax(outputs , dim=-1)\n","            total += len(inputs)\n","            correct += (pred == labels).sum().item()\n","        \n","    return loss / len(dataset) , correct / total\n","  \n","\n","def train_model(dataset_train , dataset_valid , batch_size , model , criterion , optimizer , num_epochs , collate_fn = None , device = None):\n","    model.to(device)\n","    dataloader_train = DataLoader(dataset_train , batch_size = batch_size , shuffle=True , collate_fn = collate_fn)\n","    dataloader_valid = DataLoader(dataset_valid , batch_size = 1, shuffle = False)\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer , num_epochs , eta_min=1e-3, last_epoch = -1)\n","    loss_train_list = []\n","    loss_valid_list = []\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        for data in dataloader_train:\n","            optimizer.zero_grad()#初期化\n","            inputs = data['inputs'].to(device)#gpu\n","            labels = data['labels'].to(device)#gpu\n","            #計算\n","            outputs = model.forward(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()#調節\n","            optimizer.step()#更新\n","        \n","        model.eval()\n","\n","        # 損失と正解率の算出\n","        loss_train , acc_train = calc_acc(model , dataset_train , device , criterion = criterion)\n","        loss_valid , acc_valid = calc_acc(model , dataset_valid , device , criterion = criterion)\n","        loss_train_list.append([loss_train , acc_train])\n","        loss_valid_list.append([loss_valid , acc_valid])\n","\n","        #パラメータ保存\n","        torch.save({'epoch': epoch , 'model_state_dict': model.state_dict() , 'optimizer_state_dict': optimizer.state_dict()} , f'checkpoint{epoch + 1}.pt')\n","\n","        #正答率誤算などの算出\n","        print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, acc_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, acc_valid: {acc_valid:.4f}') \n","        \n","        scheduler.step()\n","\n","    return {'train': loss_train, 'valid': loss_valid}"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55919,"status":"ok","timestamp":1649814162983,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"},"user_tz":-540},"id":"f5vSVETA2pL-","outputId":"aaca493e-a7f3-4f59-8e60-48836689a77b"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1, loss_train: 1.1354, acc_train: 0.4730, loss_valid: 1.1679, acc_valid: 0.4540\n","epoch: 2, loss_train: 1.1014, acc_train: 0.5056, loss_valid: 1.1678, acc_valid: 0.4540\n","epoch: 3, loss_train: 1.0520, acc_train: 0.5397, loss_valid: 1.1464, acc_valid: 0.4719\n"]}],"source":["vocab_size = len(set(word_id.values())) + 1\n","emb_size = 300\n","padding_idx = len(set(word_id.values()))\n","output_size = 4\n","hidden_size = 50\n","learning_rate = 1e-3\n","batch_size = 1\n","num_epochs = 3#testのため回数少なめ\n","\n","model = RNN(vocab_size , emb_size , padding_idx , output_size , hidden_size)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters() , lr = learning_rate)\n","\n","log = train_model(dataset_train , dataset_valid , batch_size , model , criterion , optimizer , num_epochs)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"HHAgvIA7VEPs","executionInfo":{"status":"ok","timestamp":1649814163395,"user_tz":-540,"elapsed":424,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["# https://note.nkmk.me/python-pytorch-device-to-cuda-cpu/\n","# https://runebook.dev/ja/docs/pytorch/generated/torch.nn.bcewithlogitsloss\n","# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n","# https://tips-memo.com/python-diff-bce"]},{"cell_type":"markdown","metadata":{"id":"v2PtIrKWJcmi"},"source":["# ミニバッチ化・GPU上での学習\n","## 問題82のコードを改変し，B事例ごとに損失・勾配を計算して学習を行えるようにせよ（Bの値は適当に選べ）．また，GPU上で学習を実行せよ．"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"XJZk_XGx7m87","executionInfo":{"status":"ok","timestamp":1649814163396,"user_tz":-540,"elapsed":3,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["class PadSequence():\n","    def __init__(self, padding_idx):\n","        self.padding_idx = padding_idx\n","\n","    def __call__(self, batch):\n","        sorted_batch = sorted(batch, key=lambda x: x['inputs'].shape[0], reverse=True)\n","        sequences = [x['inputs'] for x in sorted_batch]\n","        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=self.padding_idx)\n","        labels = torch.LongTensor([x['labels'] for x in sorted_batch])\n","\n","        return {'inputs': sequences_padded, 'labels': labels}"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16529,"status":"ok","timestamp":1649814179922,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"},"user_tz":-540},"id":"1YYLgaRh8WpJ","outputId":"5eb5b76e-4755-48e9-b947-7864e34b849d"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1, loss_train: 1.7309, acc_train: 0.1406, loss_valid: 1.7336, acc_valid: 0.1331\n","epoch: 2, loss_train: 1.6865, acc_train: 0.1473, loss_valid: 1.6871, acc_valid: 0.1388\n","epoch: 3, loss_train: 1.6444, acc_train: 0.1559, loss_valid: 1.6431, acc_valid: 0.1474\n"]}],"source":["vocab_size = len(set(word_id.values())) + 1\n","emb_size = 300\n","padding_idx = len(set(word_id.values()))\n","output_size = 4\n","hidden_size = 50\n","learning_rate = 1e-3\n","batch_size = 50\n","num_epochs = 3#testのため回数少なめ\n","\n","model = RNN(vocab_size , emb_size , padding_idx , output_size , hidden_size)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters() , lr = learning_rate)\n","# device = torch.device('cuda')#gpu\n","device = torch.device('cpu')#gpuに制限がかかったのでcpu\n","\n","log = train_model(dataset_train , dataset_valid , batch_size , model , criterion , optimizer , num_epochs , collate_fn = PadSequence(padding_idx) , device = device)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"j8myAgh79Bn9","executionInfo":{"status":"ok","timestamp":1649814179922,"user_tz":-540,"elapsed":17,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["# 参考https://atmarkit.itmedia.co.jp/ait/articles/2008/28/news030.html"]},{"cell_type":"markdown","metadata":{"id":"-HVipbmRJcoz"},"source":["# 単語ベクトルの導入\n","## 事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル）で単語埋め込みemb(x)を初期化し，学習せよ．"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"t9DUNeZMKxHc","executionInfo":{"status":"ok","timestamp":1649814179923,"user_tz":-540,"elapsed":17,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["# ダウンロード制限がかかっているのでprthを指定する\n","# ! pip install --upgrade gdown\n","# import gdown\n","# gdown.download('https://drive.google.com/u/0/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download', './GoogleNews-vectors-negative300.bin.gz', quiet=False)\n","# #model\n","# from gensim.models import KeyedVectors\n","\n","# # 学習済みモデルのロード\n","# model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz' , binary=True)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"YDSGA8QAVQt9","executionInfo":{"status":"ok","timestamp":1649814332690,"user_tz":-540,"elapsed":152783,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["# 直接pathを指定する場合\n","from gensim.models import KeyedVectors\n","model_kv = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Tutorial/GoogleNews-vectors-negative300.bin.gz' , binary=True)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"DMMLdQGXtADB","executionInfo":{"status":"ok","timestamp":1649814332692,"user_tz":-540,"elapsed":16,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["import numpy as np\n","\n","vocab_size = len(set(word_id.values())) + 1\n","emb_size = 300\n","weights = np.zeros((vocab_size , emb_size))\n","wordataset_in_pretrained = 0\n","for idx , word in enumerate(word_id.keys()):\n","    if KeyError:\n","        weights[idx] = np.random.normal(    loc   = 0 , scale = 1 , size=(emb_size ,))#正規化\n","    else:\n","        weights[idx] = model_kv[word]\n","        wordataset_in_pretrained += 1\n","weights = torch.from_numpy(weights.astype((np.float32)))#torch"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"Zm1HSzW6v_QK","executionInfo":{"status":"ok","timestamp":1649814332692,"user_tz":-540,"elapsed":14,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["class RNN(nn.Module):\n","    def __init__(self , vocab_size , emb_size , padding_idx , output_size , hidden_size , num_layers , emb_weights = None , bidirectional = False):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.num_directions = bidirectional + 1\n","\n","        if emb_weights != None:\n","            self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n","        else:\n","            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","\n","        self.rnn = nn.RNN(emb_size , hidden_size , num_layers , nonlinearity = 'relu' , bidirectional = bidirectional , batch_first = True)\n","        self.fc = nn.Linear(hidden_size * self.num_directions , output_size)\n","        \n","    def forward(self, x):\n","        self.batch_size = x.size()[0]\n","        hidden = self.init_hidden()\n","        emb = self.emb(x)\n","        out, hidden = self.rnn(emb, hidden)\n","        out = self.fc(out[: , -1 , :])\n","        return out\n","        \n","    def init_hidden(self):\n","        hidden = torch.zeros(self.num_layers * self.num_directions , self.batch_size , self.hidden_size)\n","        return hidden"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17004,"status":"ok","timestamp":1649814349682,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"},"user_tz":-540},"id":"YhaFJc6NwB7h","outputId":"b756f08b-a639-411f-9293-13ccbe6bde9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1, loss_train: 1.3931, acc_train: 0.3013, loss_valid: 1.3925, acc_valid: 0.2920\n","epoch: 2, loss_train: 1.3673, acc_train: 0.3238, loss_valid: 1.3659, acc_valid: 0.3206\n","epoch: 3, loss_train: 1.3462, acc_train: 0.3445, loss_valid: 1.3441, acc_valid: 0.3377\n"]}],"source":["vocab_size = len(set(word_id.values())) + 1\n","emb_size = 300\n","padding_idx = len(set(word_id.values()))\n","output_size = 4\n","hidden_size = 50\n","num_layers = 1\n","learning_rate = 1e-3\n","batch_size = 50\n","num_epochs = 3#testのため回数少なめ\n","\n","model = RNN(vocab_size , emb_size , padding_idx , output_size , hidden_size , num_layers , emb_weights = weights)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters() , lr = learning_rate)\n","# device = torch.device('cuda')#gpu\n","device = torch.device('cpu')#gpuに制限がかかったのでcpu\n","\n","log = train_model(dataset_train , dataset_valid , batch_size , model , criterion , optimizer , num_epochs , collate_fn = PadSequence(padding_idx) , device = device)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"S7F_W9ROw4kM","executionInfo":{"status":"ok","timestamp":1649814349682,"user_tz":-540,"elapsed":12,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["# 参考https://www.sejuku.net/blog/73026\n","# https://note.nkmk.me/python-numpy-dtype-astype/"]},{"cell_type":"markdown","metadata":{"id":"iVxcSaQHJcq3"},"source":["# 双方向RNN・多層化\n","## \n","順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．\n","\n","h⃖ T+1=0,h⃖ t=RNN←−−−(emb(xt),h⃖ t+1),y=softmax(W(yh)[h→T;h⃖ 1]+b(y))\n","ただし，h→t∈ℝdh,h⃖ t∈ℝdhはそれぞれ，順方向および逆方向のRNNで求めた時刻tの隠れ状態ベクトル，RNN←−−−(x,h)は入力xと次時刻の隠れ状態hから前状態を計算するRNNユニット，W(yh)∈ℝL×2dhは隠れ状態ベクトルからカテゴリを予測するための行列，b(y)∈ℝLはバイアス項である．また，[a;b]はベクトルaとbの連結を表す。\n","\n","さらに，双方向RNNを多層化して実験せよ．"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65414,"status":"ok","timestamp":1649814415085,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"},"user_tz":-540},"id":"z5P4B_uEKwSJ","outputId":"f59abfae-873c-4ea3-8988-f00c3ab0e8c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1, loss_train: 1.3950, acc_train: 0.1282, loss_valid: 1.3948, acc_valid: 0.1288\n","epoch: 2, loss_train: 1.3771, acc_train: 0.1855, loss_valid: 1.3770, acc_valid: 0.1894\n","epoch: 3, loss_train: 1.3606, acc_train: 0.2946, loss_valid: 1.3608, acc_valid: 0.3028\n"]}],"source":["vocab_size = len(set(word_id.values())) + 1\n","emb_size = 300\n","padding_idx = len(set(word_id.values()))\n","output_size = 4\n","hidden_size = 50\n","num_layers = 5#ここで多層化\n","learning_rate = 1e-3\n","batch_size = 50\n","num_epochs = 3#testのため回数少なめ\n","\n","model = RNN(vocab_size , emb_size , padding_idx , output_size , hidden_size , num_layers , emb_weights = weights , bidirectional = True)#bidirectional = Trueで双方向\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters() , lr = learning_rate)\n","# device = torch.device('cuda')#gpu\n","device = torch.device('cpu')#gpuに制限がかかったのでcpu\n","\n","log = train_model(dataset_train , dataset_valid , batch_size , model , criterion , optimizer , num_epochs , collate_fn = PadSequence(padding_idx) , device = device)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"4orHQA2b2hR9","executionInfo":{"status":"ok","timestamp":1649814415086,"user_tz":-540,"elapsed":13,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["# 参考https://qiita.com/m__k/items/78a5125d719951ca98d3\n","# https://axa.biopapyrus.jp/deep-learning/rnn/brnn.html\n","# https://deepage.net/deep_learning/2017/05/23/recurrent-neural-networks.html\n","# https://teratail.com/questions/185713\n","# https://qiita.com/tetsuro_skiing/items/87c0c37cefd7b601f974"]},{"cell_type":"markdown","metadata":{"id":"wMhQ_gsMJctM"},"source":["#  畳み込みニューラルネットワーク (CNN)\n","## ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列xからカテゴリyを予測するモデルを実装せよ．\n","\n","ただし，畳み込みニューラルネットワークの構成は以下の通りとする．\n","\n","単語埋め込みの次元数: dw\n","畳み込みのフィルターのサイズ: 3 トークン\n","畳み込みのストライド: 1 トークン\n","畳み込みのパディング: あり\n","畳み込み演算後の各時刻のベクトルの次元数: dh\n","畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文をdh次元の隠れベクトルで表現\n","すなわち，時刻tの特徴ベクトルpt∈ℝdhは次式で表される．\n","\n","pt=g(W(px)[emb(xt−1);emb(xt);emb(xt+1)]+b(p))\n","ただし，W(px)∈ℝdh×3dw,b(p)∈ℝdhはCNNのパラメータ，gは活性化関数（例えばtanhやReLUなど），[a;b;c]はベクトルa,b,cの連結である．なお，行列W(px)の列数が3dwになるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．\n","\n","最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトルc∈ℝdhを求める．c[i]でベクトルcのi番目の次元の値を表すことにすると，最大値プーリングは次式で表される．\n","\n","c[i]=max1≤t≤Tpt[i]\n","最後に，入力文書の特徴ベクトルcに行列W(yc)∈ℝL×dhとバイアス項b(y)∈ℝLによる線形変換とソフトマックス関数を適用し，カテゴリyを予測する．\n","\n","y=softmax(W(yc)c+b(y))\n","なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列でyを計算するだけでよい．\n","\n"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"1SC8JKlo9xXa","executionInfo":{"status":"ok","timestamp":1649814415086,"user_tz":-540,"elapsed":12,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["# 単語埋め込みの次元数: dw\n","# 畳み込みのフィルターのサイズ: 3 トークン\n","# 畳み込みのストライド: 1 トークン\n","# 畳み込みのパディング: あり\n","# 畳み込み演算後の各時刻のベクトルの次元数: dh"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"lZ_UFaz-Kvo-","executionInfo":{"status":"ok","timestamp":1649814415086,"user_tz":-540,"elapsed":12,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["from torch.nn import functional as F\n","\n","class CNN(nn.Module):\n","    def __init__(self , vocab_size , emb_size , padding_idx , output_size , out_channels , kernel_heights , stride , padding , emb_weights = None):\n","        super().__init__()\n","        if emb_weights != None:\n","            self.emb = nn.Embedding.from_pretrained(emb_weights , padding_idx = padding_idx)\n","        else:\n","            self.emb = nn.Embedding(vocab_size , emb_size , padding_idx = padding_idx)\n","        self.conv = nn.Conv2d(1 , out_channels , (kernel_heights , emb_size) , stride , (padding , 0))\n","        self.drop = nn.Dropout(0.3)\n","        self.fc = nn.Linear(out_channels, output_size)\n","        \n","    def forward(self, x):\n","        emb = self.emb(x).unsqueeze(1)\n","        conv = self.conv(emb)\n","        act = F.relu(conv.squeeze(3))\n","        max_pool = F.max_pool1d(act, act.size()[2])\n","        out = self.fc(self.drop(max_pool.squeeze(2)))\n","        return out"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"slTiYPPo--EV","executionInfo":{"status":"ok","timestamp":1649814415087,"user_tz":-540,"elapsed":13,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["vocab_size = len(set(word_id.values())) + 1\n","emb_size = 300\n","padding_idx = len(set(word_id.values()))\n","output_size = 4\n","#CNNのパラメータ\n","out_channels =100\n","kernel_heights = 3\n","stride = 1\n","padding = 1\n","\n","model = CNN(vocab_size , emb_size , padding_idx , output_size , out_channels , kernel_heights , stride , padding , emb_weights = weights)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1649814415087,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"},"user_tz":-540},"id":"0mwy7kvC_vId","outputId":"bd91ef96-7dec-4eac-aaff-4bdbb1363c22"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2064, 0.6364, 0.0472, 0.1100]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.1013, 0.6544, 0.1147, 0.1296]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.1194, 0.5507, 0.1897, 0.1402]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.1603, 0.3877, 0.1870, 0.2650]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2837, 0.3971, 0.1284, 0.1907]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2968, 0.3562, 0.2776, 0.0695]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.0967, 0.2264, 0.5557, 0.1212]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.1154, 0.3944, 0.3041, 0.1861]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.1246, 0.4816, 0.2696, 0.1243]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.1128, 0.5206, 0.1316, 0.2350]], grad_fn=<SoftmaxBackward0>)\n"]}],"source":["for num in range(10):\n","    X = dataset_train[num]['inputs']\n","    print(torch.softmax(model(X.unsqueeze(0)) , dim=-1))"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"jhOnOGt06Ery","executionInfo":{"status":"ok","timestamp":1649814415088,"user_tz":-540,"elapsed":12,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["# 参考https://qiita.com/shu_marubo/items/70b20c3a6c172aaeb8de\n","# https://qiita.com/mathlive/items/8e1f9a8467fff8dfd03c\n","# https://exture-ri.com/2021/01/11/pytorch-cnn/\n","# https://qiita.com/m__k/items/6c39cfe7dfa99102fa8e\n","# https://kento1109.hatenablog.com/entry/2019/09/30/115139"]},{"cell_type":"markdown","metadata":{"id":"Ac0_YmHBJcvP"},"source":["# 確率的勾配降下法によるCNNの学習\n","## 確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21507,"status":"ok","timestamp":1649814436583,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"},"user_tz":-540},"id":"7BoDooPEKvN1","outputId":"ad9c1b55-4032-4bfd-ca71-5c75d4756134"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1, loss_train: 1.1990, acc_train: 0.4172, loss_valid: 1.2026, acc_valid: 0.4010\n","epoch: 2, loss_train: 1.1658, acc_train: 0.4449, loss_valid: 1.1718, acc_valid: 0.4255\n","epoch: 3, loss_train: 1.1485, acc_train: 0.4749, loss_valid: 1.1569, acc_valid: 0.4544\n"]}],"source":["vocab_size = len(set(word_id.values())) + 1\n","emb_size = 300\n","padding_idx = len(set(word_id.values()))\n","output_size = 4\n","#CNNのパラメータ\n","out_channels =100\n","kernel_heights = 3\n","stride = 1\n","padding = 1\n","learning_rate = 1e-3\n","batch_size = 50\n","num_epochs = 3#testのため回数少なめ\n","\n","model = CNN(vocab_size , emb_size , padding_idx , output_size , out_channels , kernel_heights , stride , padding , emb_weights = weights)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters() , lr = learning_rate)\n","# device = torch.device('cuda')#gpu\n","device = torch.device('cpu')#gpuに制限がかかったのでcpu\n","\n","# モデルの学習\n","log = train_model(dataset_train , dataset_valid , batch_size , model , criterion , optimizer , num_epochs , collate_fn = PadSequence(padding_idx) , device = device)"]},{"cell_type":"markdown","metadata":{"id":"d_QtLlgYJcxQ"},"source":["# パラメータチューニング\n","## 問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"]},{"cell_type":"code","source":["class RNN(nn.Module):\n","    def __init__(self , vocab_size , emb_size , padding_idx , output_size , hidden_size , num_layers , emb_weights = None , bidirectional = False):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.num_directions = bidirectional + 1\n","\n","        if emb_weights != None:\n","            self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n","        else:\n","            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","\n","        self.rnn = nn.RNN(emb_size , hidden_size , num_layers , nonlinearity='relu' , bidirectional = bidirectional , batch_first = True)\n","        self.fc = nn.Linear(hidden_size * self.num_directions , output_size)\n","        \n","    def forward(self, x):\n","        self.batch_size = x.size()[0]\n","        hidden = self.init_hidden()\n","        emb = self.emb(x)\n","        out, hidden = self.rnn(emb, hidden)\n","        out = self.fc(out[: , -1 , :])\n","        return out\n","        \n","    def init_hidden(self):\n","        hidden = torch.zeros(self.num_layers * self.num_directions , self.batch_size , self.hidden_size)\n","        return hidden"],"metadata":{"id":"PBgntiKP6C8C","executionInfo":{"status":"ok","timestamp":1649814436584,"user_tz":-540,"elapsed":11,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["vocab_size = len(set(word_id.values())) + 1\n","emb_size = 300\n","padding_idx = len(set(word_id.values()))\n","output_size = 4\n","hidden_size = 50\n","num_layers = 10#多い方が良い？\n","learning_rate = 1e-3#小さい方がいい？\n","batch_size = 128#適度に増やす\n","num_epochs = 25#多めに\n","\n","model = RNN(vocab_size , emb_size , padding_idx , output_size , hidden_size , num_layers , emb_weights = weights , bidirectional = True)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters() , lr = learning_rate)\n","# device = torch.device('cuda')#gpu\n","device = torch.device('cpu')#gpuに制限がかかったのでcpu\n","\n","log = train_model(dataset_train , dataset_valid , batch_size , model , criterion , optimizer , num_epochs , collate_fn = PadSequence(padding_idx) , device = device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QudC7shH6L4r","executionInfo":{"status":"ok","timestamp":1649815366559,"user_tz":-540,"elapsed":929985,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}},"outputId":"844a0902-6a01-4dda-ab7d-3937e68cdc3d"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1, loss_train: 1.3794, acc_train: 0.4217, loss_valid: 1.3793, acc_valid: 0.4219\n","epoch: 2, loss_train: 1.3733, acc_train: 0.4217, loss_valid: 1.3733, acc_valid: 0.4219\n","epoch: 3, loss_train: 1.3676, acc_train: 0.4217, loss_valid: 1.3676, acc_valid: 0.4219\n","epoch: 4, loss_train: 1.3621, acc_train: 0.4217, loss_valid: 1.3620, acc_valid: 0.4219\n","epoch: 5, loss_train: 1.3568, acc_train: 0.4217, loss_valid: 1.3568, acc_valid: 0.4219\n","epoch: 6, loss_train: 1.3517, acc_train: 0.4217, loss_valid: 1.3517, acc_valid: 0.4219\n","epoch: 7, loss_train: 1.3468, acc_train: 0.4217, loss_valid: 1.3467, acc_valid: 0.4219\n","epoch: 8, loss_train: 1.3420, acc_train: 0.4217, loss_valid: 1.3419, acc_valid: 0.4219\n","epoch: 9, loss_train: 1.3373, acc_train: 0.4217, loss_valid: 1.3372, acc_valid: 0.4219\n","epoch: 10, loss_train: 1.3328, acc_train: 0.4217, loss_valid: 1.3327, acc_valid: 0.4219\n","epoch: 11, loss_train: 1.3283, acc_train: 0.4217, loss_valid: 1.3283, acc_valid: 0.4219\n","epoch: 12, loss_train: 1.3240, acc_train: 0.4217, loss_valid: 1.3239, acc_valid: 0.4219\n","epoch: 13, loss_train: 1.3198, acc_train: 0.4217, loss_valid: 1.3197, acc_valid: 0.4219\n","epoch: 14, loss_train: 1.3156, acc_train: 0.4217, loss_valid: 1.3156, acc_valid: 0.4219\n","epoch: 15, loss_train: 1.3117, acc_train: 0.4217, loss_valid: 1.3116, acc_valid: 0.4219\n","epoch: 16, loss_train: 1.3079, acc_train: 0.4217, loss_valid: 1.3078, acc_valid: 0.4219\n","epoch: 17, loss_train: 1.3042, acc_train: 0.4217, loss_valid: 1.3041, acc_valid: 0.4219\n","epoch: 18, loss_train: 1.3005, acc_train: 0.4217, loss_valid: 1.3004, acc_valid: 0.4219\n","epoch: 19, loss_train: 1.2969, acc_train: 0.4217, loss_valid: 1.2968, acc_valid: 0.4219\n","epoch: 20, loss_train: 1.2933, acc_train: 0.4217, loss_valid: 1.2932, acc_valid: 0.4219\n","epoch: 21, loss_train: 1.2898, acc_train: 0.4217, loss_valid: 1.2897, acc_valid: 0.4219\n","epoch: 22, loss_train: 1.2863, acc_train: 0.4217, loss_valid: 1.2862, acc_valid: 0.4219\n","epoch: 23, loss_train: 1.2830, acc_train: 0.4217, loss_valid: 1.2828, acc_valid: 0.4219\n","epoch: 24, loss_train: 1.2797, acc_train: 0.4217, loss_valid: 1.2795, acc_valid: 0.4219\n","epoch: 25, loss_train: 1.2764, acc_train: 0.4217, loss_valid: 1.2763, acc_valid: 0.4219\n"]}]},{"cell_type":"code","execution_count":41,"metadata":{"id":"pNYDnFE_EfYg","executionInfo":{"status":"ok","timestamp":1649815366560,"user_tz":-540,"elapsed":15,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["# 参考https://qiita.com/nyanko-box/items/a6f50e28383a5bd0a432\n","# https://cpp-learning.com/optuna-pytorch/\n","# https://qiita.com/Yushi1958/items/cd22ade638f7e292e520\n","# https://dreamer-uma.com/pytorch-optuna-hyperparameter-tuning/\n","# http://maruo51.com/2020/08/07/optuna_pytorch/\n","# https://ichi.pro/optuna-o-shiyoshita-pytorch-haipa-parame-ta-no-chosei-4883072668892"]},{"cell_type":"markdown","metadata":{"id":"es6FbyNdJczi"},"source":["# 事前学習済み言語モデルからの転移学習\n","## 事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"VlgIx1syKuK-","executionInfo":{"status":"ok","timestamp":1649815366560,"user_tz":-540,"elapsed":6,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["# !pip install -q transformers\n","# from transformers import BertTokenizer, BertModel\n","# from torch import cuda"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"rfL9OG1rEehF","executionInfo":{"status":"ok","timestamp":1649815366560,"user_tz":-540,"elapsed":5,"user":{"displayName":"Tatsuki Okada","userId":"12310138447043973112"}}},"outputs":[],"source":["# https://note.nkmk.me/python-pytorch-device-to-cuda-cpu/\n","# https://qiita.com/yamaru/items/63a342c844cff056a549\n","# https://qiita.com/m__k/items/e312ddcf9a3d0ea64d72\n","# https://scrapbox.io/miyamonz/pytorch,_transformers%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9FBERT%E3%81%AEfine-tuning%E3%81%AE%E6%96%B9%E6%B3%95"]}],"metadata":{"colab":{"name":"NLP_100_9.ipynb","provenance":[],"collapsed_sections":["v2PtIrKWJcmi","iVxcSaQHJcq3","wMhQ_gsMJctM","Ac0_YmHBJcvP","d_QtLlgYJcxQ","es6FbyNdJczi"],"authorship_tag":"ABX9TyPnpOc+UbaeaR62axH+qYwA"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}