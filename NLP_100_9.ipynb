{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_100_9.ipynb","provenance":[],"authorship_tag":"ABX9TyNvU9w/8kwSPdaP/M1fHAtX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# RNN, CNN\n","## ID番号への変換"],"metadata":{"id":"wy4FSmYcJIwK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBYlYY_Q7o3R"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# ID番号への変換\n","## 問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ"],"metadata":{"id":"CJIjsxfDJcfv"}},{"cell_type":"code","source":[""],"metadata":{"id":"03YyU3nTKzhG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# RNNによる予測\n","## ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列xからカテゴリyを予測するモデルとして，次式を実装せよ．\n","\n","h→0=0,h→t=RNN−→−−(emb(xt),h→t−1),y=softmax(W(yh)h→T+b(y))\n","ただし，emb(x)∈ℝdwは単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数），h→t∈ℝdhは時刻tの隠れ状態ベクトル，RNN−→−−(x,h)は入力xと前時刻の隠れ状態hから次状態を計算するRNNユニット，W(yh)∈ℝL×dhは隠れ状態ベクトルからカテゴリを予測するための行列，b(y)∈ℝLはバイアス項である（dw,dh,Lはそれぞれ，単語埋め込みの次元数，隠れ状態ベクトルの次元数，ラベル数である）．RNNユニットRNN−→−−(x,h)には様々な構成が考えられるが，典型例として次式が挙げられる．\n","\n","RNN−→−−(x,h)=g(W(hx)x+W(hh)h+b(h))\n","ただし，W(hx)∈ℝdh×dw，W(hh)∈ℝdh×dh,b(h)∈ℝdhはRNNユニットのパラメータ，gは活性化関数（例えばtanhやReLUなど）である．\n","\n","なお，この問題ではパラメータの学習を行わず，ランダムに初期化されたパラメータでyを計算するだけでよい．次元数などのハイパーパラメータは，dw=300,dh=50など，適当な値に設定せよ（以降の問題でも同様である）．"],"metadata":{"id":"RUHR6GfrJcia"}},{"cell_type":"code","source":[""],"metadata":{"id":"9xFNbtAhKyyt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 確率的勾配降下法による学習Permalink\n","## 確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"],"metadata":{"id":"Ybr_R3_0Jckm"}},{"cell_type":"code","source":[""],"metadata":{"id":"7Plx2-1lKyPm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ミニバッチ化・GPU上での学習\n","## 問題82のコードを改変し，B事例ごとに損失・勾配を計算して学習を行えるようにせよ（Bの値は適当に選べ）．また，GPU上で学習を実行せよ．"],"metadata":{"id":"v2PtIrKWJcmi"}},{"cell_type":"code","source":[""],"metadata":{"id":"udFLPDwjKxpu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 単語ベクトルの導入\n","## 事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル）で単語埋め込みemb(x)を初期化し，学習せよ．"],"metadata":{"id":"-HVipbmRJcoz"}},{"cell_type":"code","source":[""],"metadata":{"id":"t9DUNeZMKxHc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 双方向RNN・多層化\n","## \n","順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．\n","\n","h⃖ T+1=0,h⃖ t=RNN←−−−(emb(xt),h⃖ t+1),y=softmax(W(yh)[h→T;h⃖ 1]+b(y))\n","ただし，h→t∈ℝdh,h⃖ t∈ℝdhはそれぞれ，順方向および逆方向のRNNで求めた時刻tの隠れ状態ベクトル，RNN←−−−(x,h)は入力xと次時刻の隠れ状態hから前状態を計算するRNNユニット，W(yh)∈ℝL×2dhは隠れ状態ベクトルからカテゴリを予測するための行列，b(y)∈ℝLはバイアス項である．また，[a;b]はベクトルaとbの連結を表す。\n","\n","さらに，双方向RNNを多層化して実験せよ．"],"metadata":{"id":"iVxcSaQHJcq3"}},{"cell_type":"code","source":[""],"metadata":{"id":"z5P4B_uEKwSJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#  畳み込みニューラルネットワーク (CNN)\n","## ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列xからカテゴリyを予測するモデルを実装せよ．\n","\n","ただし，畳み込みニューラルネットワークの構成は以下の通りとする．\n","\n","単語埋め込みの次元数: dw\n","畳み込みのフィルターのサイズ: 3 トークン\n","畳み込みのストライド: 1 トークン\n","畳み込みのパディング: あり\n","畳み込み演算後の各時刻のベクトルの次元数: dh\n","畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文をdh次元の隠れベクトルで表現\n","すなわち，時刻tの特徴ベクトルpt∈ℝdhは次式で表される．\n","\n","pt=g(W(px)[emb(xt−1);emb(xt);emb(xt+1)]+b(p))\n","ただし，W(px)∈ℝdh×3dw,b(p)∈ℝdhはCNNのパラメータ，gは活性化関数（例えばtanhやReLUなど），[a;b;c]はベクトルa,b,cの連結である．なお，行列W(px)の列数が3dwになるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．\n","\n","最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトルc∈ℝdhを求める．c[i]でベクトルcのi番目の次元の値を表すことにすると，最大値プーリングは次式で表される．\n","\n","c[i]=max1≤t≤Tpt[i]\n","最後に，入力文書の特徴ベクトルcに行列W(yc)∈ℝL×dhとバイアス項b(y)∈ℝLによる線形変換とソフトマックス関数を適用し，カテゴリyを予測する．\n","\n","y=softmax(W(yc)c+b(y))\n","なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列でyを計算するだけでよい．\n","\n"],"metadata":{"id":"wMhQ_gsMJctM"}},{"cell_type":"code","source":[""],"metadata":{"id":"lZ_UFaz-Kvo-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 確率的勾配降下法によるCNNの学習\n","## 確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"],"metadata":{"id":"Ac0_YmHBJcvP"}},{"cell_type":"code","source":[""],"metadata":{"id":"7BoDooPEKvN1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# パラメータチューニングPermalink\n","## 問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"],"metadata":{"id":"d_QtLlgYJcxQ"}},{"cell_type":"code","source":[""],"metadata":{"id":"4kDHWpE8Ku0N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 事前学習済み言語モデルからの転移学習Permalink\n","## 事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"],"metadata":{"id":"es6FbyNdJczi"}},{"cell_type":"code","source":[""],"metadata":{"id":"VlgIx1syKuK-"},"execution_count":null,"outputs":[]}]}