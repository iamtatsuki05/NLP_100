{"cells":[{"cell_type":"markdown","metadata":{"id":"ZhMwLOHlA0IL"},"source":["# 機械学習\n","## 本章では，Fabio Gasparetti氏が公開しているNews Aggregator Data Setを用い，ニュース記事の見出しを「ビジネス」「科学技術」「エンターテイメント」「健康」のカテゴリに分類するタスク（カテゴリ分類）に取り組む．\n","#### https://nlp100.github.io/ja/ch06.html"]},{"cell_type":"markdown","metadata":{"id":"oMLjhtwFmF29"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iamtatsuki05/NLP_100/blob/fix_all_merge/NLP_100_6.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WyuEt1gNDKDE"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"UmzFKLEiA1l5"},"source":["# データの入手・整形\n","## News Aggregator Data Setをダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．\n","\n","###ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．\n","##情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n","###抽出された事例をランダムに並び替える．\n","###抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n","###学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7C5V66iiwPlx"},"outputs":[],"source":["!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWg5OR08DI9-"},"outputs":[],"source":["!unzip NewsAggregatorDataset.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gVYDTHZS-G7v"},"outputs":[],"source":["f = open('readme.txt', 'r')\n","data = f.read()\n","print(data)"]},{"cell_type":"code","source":["!pip3 install -U polars"],"metadata":{"id":"xXZ8O_zsfq3Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zDG-NINBeiT"},"outputs":[],"source":["import pandas as pd\n","import polars as pl\n","from sklearn.model_selection import train_test_split\n","\n","# df = pd.read_csv('/content/newsCorpora.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n","df = pl.read_csv('/content/newsCorpora.csv',\n","                 has_header=False,\n","                 separator='\\t',\n","                 ignore_errors=True,\n","                 encoding=\"utf8\",\n","                 new_columns=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'],\n","                 )\n","# df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n","df = df.filter(df['PUBLISHER'].is_in(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail'])).select(['TITLE', 'CATEGORY'])\n","\n","dataset_train, other = train_test_split(df, test_size=0.2, shuffle=True, random_state=42 , stratify=df['CATEGORY'])\n","dataset_valid, dataset_test = train_test_split(other, test_size=0.5, shuffle=True, random_state=42 , stratify=other['CATEGORY'])\n","\n","print('train:', dataset_train.describe())\n","print('test:', dataset_test.describe())\n","print('valid:', dataset_valid.describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N8Ym6P0dD0fE"},"outputs":[],"source":["dataset_train.to_pandas().to_csv('./train.txt', sep='\\t', index=False)\n","dataset_test.to_pandas().to_csv('./test.txt', sep='\\t', index=False)\n","dataset_valid.to_pandas().to_csv('./valid.txt', sep='\\t', index=False)"]},{"cell_type":"markdown","metadata":{"id":"DtzSEhUTA1pL"},"source":["# 特徴量抽出\n","## 学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ． なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0858e9ChDIWD"},"outputs":[],"source":["#CATEGORY\tNews category (b =business, t = science and technology, e = entertainment, m = health)\n","# df = pd.concat([dataset_train, dataset_test, dataset_valid])\n","df = pl.concat([dataset_train, dataset_test, dataset_valid])\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TKoTVSZsEv2A"},"outputs":[],"source":["import string\n","import re\n","\n","#ID修正\n","# df.reset_index(drop=True, inplace=True)\n","df = df.with_row_count()\n","#小文字化\n","# df['TITLE'] = df['TITLE'].str.lower()\n","df.with_columns(df['TITLE'].str.to_lowercase())\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JACBwHWgi6m"},"outputs":[],"source":["# train, test, valid = df[:len(train)], df[len(train):len(train) + len(test)], df[len(train) + len(test):] \n","\n","## trainとvalidは同じ状態で前処理を行って直前に直前に分けたほうが良いここで時間がかかった\n","\n","# #文章のベクトル化\n","# from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# vec_tfidf = TfidfVectorizer(min_df=10, ngram_range=(1, 2))\n","\n","# train_title = vec_tfidf.fit_transform(train['TITLE'])\n","# test_title = vec_tfidf.fit_transform(test['TITLE'])\n","# valid_title = vec_tfidf.fit_transform(valid['TITLE'])\n","\n","# X_train = pd.DataFrame(train_title.toarray())\n","# X_test = pd.DataFrame(test_title.toarray())\n","# X_valid = pd.DataFrame(valid_title.toarray())\n","# X_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kW6DC19eaFMF"},"outputs":[],"source":["#文章のベクトル化\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vec_tfidf = TfidfVectorizer(min_df=10, ngram_range=(1, 2))\n","df_title = vec_tfidf.fit_transform(df['TITLE'])\n","\n","# df = pl.DataFrame(df_title.toarray(), columns=vec_tfidf.get_feature_names_out())\n","df = pl.DataFrame(df_title.toarray(), schema=list(vec_tfidf.get_feature_names_out()))\n","X_train, X_test, X_valid = df[:len(dataset_train)], df[len(dataset_train):len(dataset_train) + len(dataset_test)], df[len(dataset_train) + len(dataset_test):]\n","X_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m8fxTeTOEv9p"},"outputs":[],"source":["# 参考https://qiita.com/Yuu94/items/ca1ad7eb1dd5c9686c5a\n","# https://qiita.com/tag1216/items/df6c93bdb823dd48af6c\n","# https://pyhoo.jp/upper-lower"]},{"cell_type":"markdown","metadata":{"id":"V5IacBzaA1w5"},"source":["# 学習\n","## 51で構築した学習データを用いて，ロジスティック回帰モデルを学習せよ．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5bd_whUDHll"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","\n","lg = LogisticRegression(penalty='l2', solver='sag', random_state=42, max_iter=100)\n","lg.fit(X_train, dataset_train['CATEGORY'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pKg98reFV7HL"},"outputs":[],"source":["# https://qiita.com/fujin/items/f5656afc8a40fcf55386"]},{"cell_type":"markdown","metadata":{"id":"kTGKDqwOA1zq"},"source":["# 予測\n","## 52で学習したロジスティック回帰モデルを用い，与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"voY71Prs2KT3"},"outputs":[],"source":["train_pred = lg.predict(X_train)\n","train_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HoBm6pyHVxie"},"outputs":[],"source":["test_pred = lg.predict(X_test)\n","test_pred"]},{"cell_type":"markdown","metadata":{"id":"0uFKaLhpA12u"},"source":["# 正解率の計測\n","## 52で学習したロジスティック回帰モデルの正解率を，学習データおよび評価データ上で計測せよ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7KdJKSGaDF9D"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","train_accuracy = accuracy_score(dataset_train['CATEGORY'], train_pred)\n","print(f'score：{train_accuracy:.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tirncHvjcKSx"},"outputs":[],"source":["test_accuracy = accuracy_score(dataset_test['CATEGORY'], test_pred)\n","print(f'score：{test_accuracy:.3f}')"]},{"cell_type":"markdown","metadata":{"id":"qT1fVZx3A151"},"source":["# 混同行列の作成\n","## 52で学習したロジスティック回帰モデルの混同行列（confusion matrix）を，学習データおよび評価データ上で作成せよ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NXx0NwmMDFcX"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","train_cm = confusion_matrix(dataset_train['CATEGORY'], train_pred)\n","print(train_cm)\n","sns.heatmap(train_cm, cmap='Greens')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHMbEADzce6I"},"outputs":[],"source":["test_cm = confusion_matrix(dataset_test['CATEGORY'], test_pred)\n","print(test_cm)\n","sns.heatmap(test_cm, cmap='Greens')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M2sWsTlDc32w"},"outputs":[],"source":["# 参考https://note.nkmk.me/python-sklearn-confusion-matrix-score/"]},{"cell_type":"markdown","metadata":{"id":"dP_cxZPSA18q"},"source":["# 適合率，再現率，F1スコアの計測\n","## 52で学習したロジスティック回帰モデルの適合率，再現率，F1スコアを，評価データ上で計測せよ．カテゴリごとに適合率，再現率，F1スコアを求め，カテゴリごとの性能をマイクロ平均（micro-average）とマクロ平均（macro-average）で統合せよ．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AoEiSb1akzwH"},"outputs":[],"source":["# from sklearn.metrics import precision_score, recall_score, f1_score\n","# import numpy as np\n","\n","# # 適合率\n","# precision = precision_score(test['CATEGORY'], test_pred, average=None, labels=['b', 'e', 't', 'm'])\n","# precision = np.append(precision, precision_score(test['CATEGORY'], test_pred, average='micro'))\n","# precision = np.append(precision, precision_score(test['CATEGORY'], test_pred, average='macro'))\n","\n","# # 再現率\n","# recall = recall_score(test['CATEGORY'], test_pred, average=None, labels=['b', 'e', 't', 'm'])\n","# recall = np.append(recall, recall_score(test['CATEGORY'], test_pred, average='micro'))\n","# recall = np.append(recall, recall_score(test['CATEGORY'], test_pred, average='macro'))\n","\n","# # F1スコア\n","# f1 = f1_score(test['CATEGORY'], test_pred, average=None, labels=['b', 'e', 't', 'm'])\n","# f1 = np.append(f1, f1_score(test['CATEGORY'], test_pred, average='micro'))\n","# f1 = np.append(f1, f1_score(test['CATEGORY'], test_pred, average='macro'))\n","\n","# score_dataframe = pd.DataFrame({'適合率': precision, '再現率': recall, 'F1スコア': f1},\n","#                         index=['b', 'e', 't', 'm', 'マイクロ平均', 'マクロ平均'])\n","# score_dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"La8rgvSESbDa"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","# target_names = ['b', 'e', 'm', 't']\n","print('test_data\\n')\n","print(classification_report(dataset_test['CATEGORY'], test_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hnMIyLdPDE5r"},"outputs":[],"source":["# 参考https://qiita.com/g-k/items/14bf10cce79f0db72b24\n","# https://qiita.com/shoku-pan/items/cef42c9a3f472eb571b9\n","# https://note.nkmk.me/python-sklearn-confusion-matrix-score/\n","# https://gotutiyan.hatenablog.com/entry/2020/09/09/111840"]},{"cell_type":"markdown","metadata":{"id":"ECUALp4WA1_j"},"source":["# 特徴量の重みの確認Permalink\n","## 52で学習したロジスティック回帰モデルの中で，重みの高い特徴量トップ10と，重みの低い特徴量トップ10を確認せよ．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wl7zA7r4DEVN"},"outputs":[],"source":["import numpy as np\n","\n","# cols = X_train.columns.values\n","cols = np.array(X_train.columns)\n","index = [_ for _ in range(1, 11)]\n","for category, score in zip(lg.classes_, lg.coef_):\n","    print(f'【カテゴリ】{category}')\n","    # high = pd.DataFrame(cols[np.argsort(score)[::-1][:10]], columns=['重みの高い特徴量'], index=index)\n","    # low = pd.DataFrame(cols[np.argsort(score)[:10]], columns=['重みの低い特徴量'], index=index)\n","    # frame = pd.concat([high, low], axis=1)\n","    high = pl.DataFrame(cols[np.argsort(score)[::-1][:10]], schema=['重みの高い特徴量'])\n","    low = pl.DataFrame(cols[np.argsort(score)[:10]], schema=['重みの低い特徴量'])\n","    frame = pl.concat([high, low], how=\"horizontal\")\n","    print(frame)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vNx5m2PpGHX"},"outputs":[],"source":["# 参考https://gotutiyan.hatenablog.com/entry/2020/09/08/122621#classes_%E3%81%A9%E3%81%AE%E3%83%A9%E3%83%99%E3%83%AB%E3%81%8C%E3%81%A9%E3%81%AEID%E3%81%AA%E3%81%AE%E3%81%8B%E3%82%92%E5%8F%96%E5%BE%97\n","# https://qiita.com/rubberduck/items/c2ade1ac10c80651f4b1"]},{"cell_type":"markdown","metadata":{"id":"g3xNFZvMA2Cu"},"source":["# 正則化パラメータの変更\n","## ロジスティック回帰モデルを学習するとき，正則化パラメータを調整することで，学習時の過学習（overfitting）の度合いを制御できる．異なる正則化パラメータでロジスティック回帰モデルを学習し，学習データ，検証データ，および評価データ上の正解率を求めよ．実験の結果は，正則化パラメータを横軸，正解率を縦軸としたグラフにまとめよ．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AoCQOiSSwp31"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","\n","result = []\n","Cs = [10 ** i for i in range(-3, 3)]\n","\n","for C in Cs:\n","    lg = LogisticRegression(penalty='l2', solver='sag', random_state=42, max_iter=100, C=C)\n","    lg.fit(X_train, dataset_train['CATEGORY'])\n","    # 予測\n","    train_pred = lg.predict(X_train)\n","    valid_pred = lg.predict(X_valid)\n","    test_pred = lg.predict(X_test)\n","\n","    # 誤差\n","    train_accuracy = accuracy_score(dataset_train['CATEGORY'], train_pred)\n","    valid_accuracy = accuracy_score(dataset_valid['CATEGORY'], valid_pred)\n","    test_accuracy = accuracy_score(dataset_test['CATEGORY'], test_pred)\n","\n","    # 結果\n","    result.append([C, train_accuracy, valid_accuracy, test_accuracy])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GYQ1djZtv-CI"},"outputs":[],"source":["result = np.array(result).T\n","plt.plot(result[0], result[1], label='train')\n","plt.plot(result[0], result[2], label='valid')\n","plt.plot(result[0], result[3], label='test')\n","plt.ylim(0, 1.1)\n","plt.ylabel('score')\n","plt.xlabel('Normalization')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMrPQNTkxfVQ"},"outputs":[],"source":["# 参考http://harmonizedai.com/article/%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%83%BC-c%E3%80%80%E3%83%AD%E3%82%B8%E3%82%B9%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E5%9B%9E%E5%B8%B0%E3%81%AE%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91/\n","# https://machine-earning.net/article/numpy-logspace/"]},{"cell_type":"markdown","metadata":{"id":"rPCMdcz9A2Fj"},"source":["# ハイパーパラメータの探索\n","## 学習アルゴリズムや学習パラメータを変えながら，カテゴリ分類モデルを学習せよ．検証データ上の正解率が最も高くなる学習アルゴリズム・パラメータを求めよ．また，その学習アルゴリズム・パラメータを用いたときの評価データ上の正解率を求めよ．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBTuTf0ADDNV"},"outputs":[],"source":["import itertools\n","\n","def calc_scores(C,class_weight):\n","    model = LogisticRegression(random_state=42, max_iter=100, C=C, class_weight=class_weight)\n","    model.fit(X_train, dataset_train['CATEGORY'])\n","\n","    train_pred = model.predict(X_train)\n","    valid_pred = model.predict(X_valid)\n","    test_pred = model.predict(X_test)\n","\n","    scores = []\n","    scores.append(accuracy_score(dataset_train['CATEGORY'], train_pred))\n","    scores.append(accuracy_score(dataset_valid['CATEGORY'], valid_pred))\n","    scores.append(accuracy_score(dataset_test['CATEGORY'], test_pred))\n","    return scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2p7Smtx4SsHc"},"outputs":[],"source":["#定義\n","cs = [10 ** _ for _ in range(-3, 3)]\n","class_weight = [None, 'balanced']\n","best_parameter = None\n","best_scores = None\n","max_valid_score = 0\n","\n","#探索\n","for c, w in itertools.product(cs, class_weight):\n","    print(c, w)\n","    scores = calc_scores(c, w)\n","    if scores[1] > max_valid_score:\n","        max_valid_score = scores[1]\n","        best_parameter = [c, w]\n","        best_scores = scores\n","\n","#結果\n","print (f'Normalization: {best_parameter[0]} weight: {best_parameter[1]}')\n","print (f'train score: {best_scores[0]} valid score: {best_scores[1]} test score: {best_scores[2]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbGMIpM2XhMf"},"outputs":[],"source":["# lightgbm\n","from lightgbm import LGBMRegressor\n","\n","category_dict = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n","# y_train = dataset_train['CATEGORY'].map(lambda x: category_dict[x]).values\n","# y_valid = dataset_valid['CATEGORY'].map(lambda x: category_dict[x]).values\n","# y_test = dataset_test['CATEGORY'].map(lambda x: category_dict[x]).values\n","y_train = dataset_train['CATEGORY'].apply(lambda x: category_dict[x])\n","y_valid = dataset_valid['CATEGORY'].apply(lambda x: category_dict[x])\n","y_test = dataset_test['CATEGORY'].apply(lambda x: category_dict[x])\n","model = LGBMRegressor(random_state=42, n_jobs=-1, n_estimators=1000)\n","model.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wItqKhDHcNc0"},"outputs":[],"source":["model.score(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-873v5IOcZ70"},"outputs":[],"source":["model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"40bU1H2ifa_i"},"outputs":[],"source":["model.score(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jgv0f0h7ckwE"},"outputs":[],"source":["model.predict(X_valid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eO6bTOrofhOI"},"outputs":[],"source":["model.score(X_valid, y_valid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GF_qeX3-zK8_"},"outputs":[],"source":["# 参考https://docs.python.org/ja/3/library/itertools.html\n","# https://techacademy.jp/magazine/46131\n","# https://qiita.com/kimisyo/items/f7755eb6846193e3ae23\n","# https://datadriven-rnd.com/lightgbm/"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}